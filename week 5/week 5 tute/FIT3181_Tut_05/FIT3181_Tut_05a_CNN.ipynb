{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (2022)</span>\n",
    "***\n",
    "*CE/Lecturer (Clayton):*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Lecturer (Malaysia):*  **Dr Lim Chern Hong** | lim.chernhong@monash.edu <br/>  <br/>\n",
    "*Tutor:*  **Mr Thanh Nguyen** \\[Thanh.Nguyen4@monash.edu \\] |**Mr Tuan Nguyen**  \\[tuan.ng@monash.edu \\] |**Mr Anh Bui** \\[tuananh.bui@monash.edu\\] | **Dr Binh Nguyen** \\[binh.nguyen1@monash.edu \\] | **Mr Md Mohaimenuzzaman** \\[md.mohaimen@monash.edu \\] |**Mr James Tong** \\[james.tong1@monash.edu \\]\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***# <span style=\"color:#0b486b\">  FIT5215: Deep Learning (2022)</span>\n",
    "***\n",
    "*CE/Lecturer:*  **Dr Trung Le** | trunglm@monash.edu <br/> <br/>\n",
    "*Tutor:*  **Mr Tuan Nguyen**  \\[tuan.ng@monash.edu \\] |**Mr Anh Bui** \\[tuananh.bui@monash.edu\\] | **Mr Xiaohao Yang** \\[xiaohao.yang@monash.edu \\] | **Mr Md Mohaimenuzzaman** \\[md.mohaimen@monash.edu \\] |**Mr Thanh Nguyen** \\[Thanh.Nguyen4@monash.edu \\] |\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RECAP: \n",
    "pooling lyaer (max OR avg pooling) shrinks it down number of inputs to half of the size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Tutorial 4a: CNN Fundamentals</span> <span style=\"color:red;  font-size: 18px\">***** (highly important)</span> #\n",
    "\n",
    "**The purpose of this tutorial is to demonstrate two fundamental layers in CNN, namely convolutional and max-pooling layers. The following topics will be covered:**\n",
    "1. How to declare a `convolutional layer` with TensorFlow and how it works\n",
    "2. How to declare a `max-pooling layer` with TensorFlow and how it works\n",
    "\n",
    "*Acknowledgement: this tutorial was developed based non Chapter 14 materials from the book `Hands-on Machine Learning with Scikit-learn and Tensorflow (TF 2.x edition)`.*\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">I. Preparation</span> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.1.  Setup </span> ###\n",
    "We make sure the code is compatible with both Python 2 and 3. Also, we import some basic modules and do some settings for `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images\n",
    "def plot_image(image, scale=False, axis=False):\n",
    "    if scale:\n",
    "        plt.rcParams[\"figure.figsize\"] = (image.shape[0]/50.0, image.shape[1]/50.0)\n",
    "    if not axis:\n",
    "        plt.axis(\"off\")\n",
    "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.2. Load two sample images using sklearn</span> ###\n",
    "We now load two images using `sklearn`. These two images will be used in the sequel to demonstrate the effects of `convolutional` and `max pooling` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "china = load_sample_image(\"china.jpg\")[80:360, 70:390]\n",
    "flower = load_sample_image(\"flower.jpg\")[80:360, 130:450]\n",
    "plot_image(china)\n",
    "plot_image(flower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">II. Convolution Layer </span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">II.1. Declare a convolutional layer with TF</span> ###\n",
    "We use the class `tf.nn.conv2d` with the following syntax `tf.nn.conv2d(input, filter, strides, padding, name)`.\n",
    "- `input` is a 4-D tensor of shape $[batch, in\\_height, in\\_width, in\\_channels]$.\n",
    "  - batch: batch size \n",
    "  - input matrix height (image pixels)\n",
    "  - input matrix width\n",
    "  - channels (color depth of the image)\n",
    "- `filter` is a tensor of shape $[filter\\_height, filter\\_width, in\\_channels, out\\_channels]$.\n",
    "  - out channels: the depth of the next layer after passing through the filter ; based on how may filters we provide\n",
    "- `strides` is a four-element 1D array [batch_size, batch_h, batch_width depth] , where the two central elements are the vertical and horizontal strides ($s_h$ and $s_w$). The first and last elements must currently be equal to 1. They may one day be used to specify a batch stride (to skip some instances) and a channel stride (to skip some of the previous layer’s feature maps or channels).\n",
    "  - strides is the gap in-between the batches ; if stride == 1, then there really is no gap in between\n",
    "  - RECAP: if stride == height & width, then there is no overlap between the \"batches\"\n",
    "- `padding` must be either `SAME` or `VALID`.\n",
    " - If set to `VALID`, the convolutional layer does not use zero-padding, and **may ignore some rows and columns at the bottom and right of the input image**, depending on the stride.\n",
    " - If set to `SAME`, the convolutional layer **uses zero-padding if necessary**. In this case, the number of output neurons is equal to the number of input neurons divided by the stride, rounded up (in this example, ceil (13 / 5) = 3). Then zeros are added as evenly as possible around the inputs.\n",
    "\n",
    "<img src=\"./images/Conv2D_HandOns.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">II.2. VALID and SAME padding</span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with examining the `VALID` and `SAME` padding for a 1-D tensor. \n",
    "\n",
    "If padding is set to `SAME`, we pad $0$(s) to the left and right of the 1-D tensor if necessary to ensure the starting point of the last window resides in the original tensor and the last window in the padding tensor, whereas the window after the last window is outside the original tensor.\n",
    "- The output size is $output\\_size=\\lfloor\\frac{input\\_size-1}{stride}\\rfloor+1$\n",
    "\n",
    "If padding is set to `VALID`, we do not pad any $0$ and some rightmost values might be ignored, depending on the stride.\n",
    "- The output size is $output\\_size=\\lfloor\\frac{input\\_size-window\\_size}{stride}\\rfloor+1$\n",
    "\n",
    "\n",
    "<img src=\"./images/padding_same_valid.png\" width=500 align=\"center\"/>\n",
    "\n",
    "In the above figure, the input size is $13$, the stride is $3$, and the window size is $5$. Therefore, we obtain\n",
    "- If padding=`SAME`, then $output\\_size=\\lfloor\\frac{13-1}{3}\\rfloor+1=5$ and we need to pad $4*3 + 5-13=4$ zeros of which the two zeros are padded to the left and the two zeros are padded to the right.\n",
    "  - why is the padding = 4? for SAME: output size = (inputsize - window_size) / stride + 1 ; window_size = filter_size + 2 * padding\n",
    "- If padding=`VALID`, then $output\\_size=\\lfloor\\frac{13-5}{3}\\rfloor+1=3$ and we do not pad any zero.\n",
    "\n",
    "For a 2-D tensor, the principle is the same, but we need to consider two dimensions: the `height` and `width`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">II.3. An example of a convolutional layer</span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load images and create a mini-batch of two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "# Load sample images\n",
    "china = load_sample_image(\"china.jpg\")[80:360, 70:390]\n",
    "flower = load_sample_image(\"flower.jpg\")[80:360, 130:450]\n",
    "batch = np.array([china, flower], dtype=np.float32)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, height, width, channels = batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `filters` with $fiter\\_height=7, filter\\_width=7, in\\_channels=3, out\\_channels=2$. \n",
    "Our filters have two separate filters, each is a 3-D tensor of $7\\times7\\times3$.\n",
    "\n",
    "With the first filter, we set all elements on the plane: $width=3$ to $1$. That is why it is a vertical plane.\n",
    "\n",
    "With the first filter, we set all elements on the plane: $height=3$ to $1$. That is why it is a horizontal plane.\n",
    "\n",
    "We finally declare a convolutional layer to apply this filter on a mini-batch of $2$ images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ME: \n",
    "we want to use a filter to extract a pattern from the image\n",
    "we have 2 filters in this case, 1 is to extract a vertical line, another to extract the horizontal line\n",
    "\n",
    "- case study: if we take a pixel and split into 4 parts, and look : we can see that the minimum values are roughly the same in values, only the parts with maximum value shows the most difference from all other parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create 2 filters\n",
    "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
    "filters[:, 3, :, 0] = 1  # vertical line, why? \n",
    "# because the 2nd argument is stating that we want the 3rd column to be filled with 1s\n",
    "\n",
    "filters[3, :, :, 1] = 1  # horizontal line, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the `filters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(filters[:, :, 0, 0])\n",
    "plot_image(filters[:, :, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `TensorFlow 2.x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `tf.nn.conv2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.nn.conv2d(batch, filters, strides=[1,2,2,1], padding=\"SAME\")\n",
    "print(\"Output shape:\" + str(output.shape))\n",
    "\n",
    "plot_image(output[0, :, :, 0], axis=True) # plot 1st image's 1nd feature map, channel 0\n",
    "plot_image(output[0, :, :, 1], axis=True) # plot 1st image's 2nd feature map, channel 1\n",
    "plot_image(output[1, :, :, 0], axis=True) # plot 2nd image's 1nd feature map, channel 0\n",
    "plot_image(output[1, :, :, 1], axis=True) # plot 2nd image's 2nd feature map, channel 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **<span style=\"color:red\">Exercise 1</span>:** Explain why the output shape is $(2, 140, 160, 2)$. Change the parameter padding to `VALID` and record new output size. Explain the new result.\n",
    "\n",
    " ANS:\n",
    " - stride = 2 x 2 ; padding = SAME ; \n",
    " - the shrinkage in size is due to stride\n",
    " - the original size is halved\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">III. Pooling Layer </span>\n",
    "\n",
    "GOAL: we want to shrink the image into half size, and retain the most significant features from the original image, by using (MAX / AVG pooling) ; from research, it suggests that MAX pooling gets a very good result already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">III.1. Declare a pooling layer with TF</span> ###\n",
    "We use the class `tf.nn.max_pool` with the following syntax: `tf.nn.max_pool(value, ksize, strides, padding, name=None)`\n",
    "- `value` is a 4-D Tensor tensor of shape $[batch, in\\_height, in\\_width, in\\_channels]$.\n",
    "- `ksize` is a 1-D int Tensor of 4 elements which is the size of the window for each dimension of the input tensor. The first and last dimensions in `ksize` must be $1$.\n",
    "- `strides` is a 1-D int Tensor of 4 elements which is the stride of the sliding window for each dimension of the input tensor. The first and last dimensions in `strides` must be $1$.\n",
    "- `padding` must be either `VALID` or `SAME`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">III.2. An example of a pooling layer</span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\"> Example of `Max Pooling` </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.nn.max_pool(input = batch, ksize=(2,2), strides=(2,2), padding=\"SAME\")\n",
    "output = output.numpy()  # convert Eager-Execution Tensor object to numpy array\n",
    "plot_image(batch[0].astype(np.int32), scale=True) # plot the 1st original image\n",
    "plot_image(output[0].astype(np.int32), scale=True) # plot the output for the 1st image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\"> Example of `Average Pooling` </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.nn.avg_pool(input = batch, ksize=(2,2), strides=(2,2), padding=\"SAME\")\n",
    "output = output.numpy()  # convert Eager-Execution Tensor object to numpy array\n",
    "print(batch[0].shape)\n",
    "print(output[0].shape)\n",
    "plt.rcParams[\"figure.figsize\"] = (batch[0].shape[0], batch[0].shape[1])\n",
    "plot_image(batch[0].astype(np.int32), scale=True) # plot the 1st original image\n",
    "plot_image(output[0].astype(np.int32), scale=True) # plot the output for the 1st image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">IV. Additional Reading: Effects of Convolution Operation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section aims to demonstrate the effects of convolution operation in processing images. In particular, by creating appropriate filters (kernels), we can do blurring, sharpening, embossing, or detecting edges of images. We first create some specific filters for those purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">IV.1. Creating the effect filters (kernels) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter (kernel) for blurring images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallBlur = np.ones((7, 7), dtype=\"float\") * (1.0 / (7 * 7))\n",
    "largeBlur = np.ones((21, 21), dtype=\"float\") * (1.0 / (21 * 21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fiter (kernel) for sharpening images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a sharpening filter\n",
    "sharpen = np.array(([0, -1, 0],\n",
    "                    [-1, 5, -1],\n",
    "                    [0, -1, 0]), dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpen_3D = np.stack([sharpen]*3, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpen_3D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Laplacian kernel for detecting edge-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the Laplacian kernel used to detect edge-like\n",
    "# regions of an image\n",
    "laplacian = np.array(([0, 1, 0], \n",
    "                      [1, -4, 1], \n",
    "                      [0, 1, 0]), dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_3D = np.stack([laplacian]*3, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sobel kernel for detecting edge-like regions along both the x and y axis, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the Sobel x-axis kernel\n",
    "sobelX = np.array(([-1, 0, 1], \n",
    "                   [-2, 0, 2], \n",
    "                   [-1, 0, 1]), dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobelX_3D = np.stack([sobelX]*3, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobelY = np.array(([-1, -2, -1],\n",
    "                   [0, 0, 0],\n",
    "                   [1, 2, 1]), dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobelY_3D = np.stack([sobelY]*3, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Emboss kernel to emboss images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an emboss kernel\n",
    "emboss = np.array(([-2, -1, 0],\n",
    "                   [-1, 1, 1],\n",
    "                   [0, 1, 2]), dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emboss_3D = np.stack([emboss]*3, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stacking the $3 \\times 3$ effect filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_filters = np.stack([sharpen_3D, laplacian_3D, sobelX_3D, sobelY_3D, emboss_3D], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_filters.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china = load_sample_image(\"china.jpg\")[80:360, 70:390]\n",
    "china = np.array([china], dtype= np.float32)\n",
    "china.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> IV.2. Applying the small blurring kernels to the china image </span>\n",
    "\n",
    "The output image is blurred a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = np.stack([smallBlur]*3, axis=2)\n",
    "filters = np.expand_dims(filters,3)\n",
    "filters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.nn.conv2d(china, filters, strides=[1,1,1,1], padding=\"SAME\")\n",
    "output = output.numpy()\n",
    "output.shape\n",
    "plt.imshow(output[0, :, :, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> IV.3. Applying the large blurring kernels to the china image </span>\n",
    "\n",
    "The output image is largely blurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = np.stack([largeBlur]*3, axis=2)\n",
    "filters = np.expand_dims(filters,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.nn.conv2d(china, filters, strides=[1,1,1,1], padding=\"SAME\")\n",
    "output = output.numpy()\n",
    "output.shape\n",
    "plt.imshow(output[0, :, :, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> IV.3. Applying other filters to the china image </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.nn.conv2d(china, effect_filters, strides=[1,1,1,1], padding=\"SAME\")\n",
    "output = output.numpy()\n",
    "print(output.shape)\n",
    "print(\"-\"*100)\n",
    "print(\"Sharpen effect\")\n",
    "plt.imshow(output[0, :, :, 0], cmap='gray')\n",
    "plt.show()\n",
    "print(\"-\"*100)\n",
    "print(\"Lalapcian effect\")\n",
    "plt.imshow(output[0, :, :, 1], cmap='gray')\n",
    "plt.show()\n",
    "print(\"-\"*100)\n",
    "print(\"SobelX effect\")\n",
    "plt.imshow(output[0, :, :, 2], cmap='gray')\n",
    "plt.show()\n",
    "print(\"-\"*100)\n",
    "print(\"SobelY effect\")\n",
    "plt.imshow(output[0, :, :, 3], cmap='gray')\n",
    "plt.show()\n",
    "print(\"-\"*100)\n",
    "print(\"Emboss effect\")\n",
    "plt.imshow(output[0, :, :, 4], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUES: what are we learning in CNN? \n",
    "- we are learning to SELECT the best filters to apply to the transofmation of the input matrix at each layer\n",
    "- and compare it with the most fundamental layer at the input layer \n",
    "- to see which filter represents the most fundamental input layer the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf2_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "92c79073133677da89801d1b4bc42714b1a3d83cbc90a7f9c522b094b613b522"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
