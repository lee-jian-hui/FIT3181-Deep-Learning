{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (2021)</span>\n",
    "***\n",
    "*CE/Lecturer:*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Head TA:*  **Mr Thanh Nguyen** | thanh.nguyen4@monash.edu <br/>\n",
    "*Tutor:* **Dr Van Nguyen**  \\[van.nguyen1@monash.edu \\] | **Mr James Tong** \\[james.tong1@monash.edu\\] | **Dr Mahmoud Mohammad** \\[mahmoud.hossam@monash.edu\\]\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Tutorial 4: Stochastic Gradient Descent and Optimization</span> #\n",
    "**The purpose of this tutorial is to help you understand and visualize gradients of machine learning algorithms as well as the optimization of deep learning with TensorFlow. The tutorial consists of two parts:**\n",
    "\n",
    "I. Visualize the gradient of the training of a logistic regression model on a synthetic dataset\n",
    "\n",
    "II. Show the training procedure of a DNN model on the MNIST dataset\n",
    "\n",
    "**References and additional reading and resources**\n",
    "- An overview of gradient descent optimization algorithms ([link](https://ruder.io/optimizing-gradient-descent/)).\n",
    "- Logistic Regression ([link](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)).\n",
    "- TensorFlow Keras model ([link](https://www.tensorflow.org/api_docs/python/tf/keras/Model)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\"> I. Visualize the gradient of the training of a logistic regression model on a synthetic dataset </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> I.1 Introduction of logistic regression </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression can be viewed as a simple neural network for binary classification with the input and output layers.\n",
    "- The input layer takes inputs as one feature vector (data point, data instance, or data example), mini-batches of feature vectors, or entire dataset of feature vectors.\n",
    "- The output layer has one neuron and was applied with the activation function, i.e., sigmoid to obtain the predicted probability of a given data instance to be in the positive class (i.e., the class 1).\n",
    "\n",
    "We choose logistic regression to visually demonstrate the processes of forward/backward propagations for performing SGD (Stochastic Gradient Descend) updates to solve the optimization problem behind logistic regression. This would offer you a better understanding of using forward/backward propagations when training deep neural networks. With the implementation in this part, you will enjoy the learning of how to feed a mini-batch to a network, do forward propagation, define the loss function, and then perform backward propagation for computing the gradients for updating the model in an SGD manner.\n",
    "\n",
    "For the ease of comprehensibility, we present the technical details for the case of a synthetic dataset with two features (i.e., data point $x \\in \\mathbb{R}^{2}$), meaning that the input layer has two neurons. However, the implementation can be generalized to a general case with many features (i.e., data point $x \\in \\mathbb{R}^{d}$).\n",
    "\n",
    "The architecture of logistic regression is shown in the following figure.\n",
    "\n",
    "\n",
    "<img src=\"imgs/LogisticRegression.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "The model parameters of our simple logistic regression include $W=\\left[\\begin{array}{c}\n",
    "w_{1}\\\\\n",
    "w_{2}\n",
    "\\end{array}\\right]\\in\\mathbb{R}^{2\\times1}$. Here note that we do not use the bias to simplify the model for the visualizing purpose in the following steps. Let us denote the training set as $D=\\left\\{ (x^{1},y^{1}),(x^{2},y^{2})...,(x^{N},y^{N})\\right\\}$ where data point and feature vector $x^i = [x^i_1, x^i_2] \\in \\mathbb{R}^{1\\times 2}$ and label $y^i \\in \\{0,1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\"> Forward propagation </span>\n",
    "\n",
    "The computational process of logistic regression for a data point $x \\in \\mathbb{R}^{1\\times 2}$ with the label $y \\in \\{0,1\\}$ is as follows:\n",
    "- We feed $x \\in \\mathbb{R}^{1\\times2}$ to the logistic regression network.\n",
    "- We compute: $z= xW \\in \\mathbb{R}^{1\\times 1}$.\n",
    "- We apply sigmoid over $z$: $p = sigmoid(z) \\in\\mathbb{R}^{1\\times 1}$.\n",
    "\n",
    "$p$ represents the probability that $x$ is classified as positive class or equivalently $p = p(y=1 \\mid x) =sigmoid(z)= \\frac{1}{1+e^{-z}}= \\frac{1}{1+e^{-xW}}$. The loss incurred is as follows:\n",
    "- $l(x,y;W) = -y*log(p) - (1-y)*log(1-p)$. Your task is to explain why it is.\n",
    "\n",
    "The loss incurred by the entire dataset is as follows:\n",
    "- $l(W,D)=\\frac{1}{N}\\sum_{i=1}^{N}l(x^{i},y^{i};W)$\n",
    "\n",
    "To efficiently solve the above optimization problem, we apply SGD in which at each iteration we feed a mini-batch $X$ to the network and then rely on backward propagation to compute the gradient of the loss function w.r.t to $W$ for the current mini-batch.\n",
    "\n",
    "The computational process of logistic regression for a mini-batch $X \\in \\mathbb{R}^{batch\\_size\\times2}$ with labels $y \\in \\mathbb{R}^{batch\\_size\\times1}$ is as follows:\n",
    "- We feed a mini-batch $X=\\left[\\begin{array}{cc}\n",
    "b_{1}^{1} & b_{2}^{1}\\\\\n",
    "b_{1}^{2} & b_{2}^{2}\\\\\n",
    "... & ...\\\\\n",
    "b_{1}^{batch\\_size} & b_{2}^{batch\\_size}\n",
    "\\end{array}\\right]\\in\\mathbb{R}^{batch\\_size\\times2}$ of 2D feature vectors (data points) to our logistic regression network.\n",
    "- We compute $z = XW \\in \\mathbb{R}^{batch\\_size\\times 1}$.\n",
    "- We apply sigmoid over $z$: $p = sigmoid(z) \\in \\mathbb{R}^{batch\\_size\\times 1}$.\n",
    "\n",
    "The loss incurred by the current mini-batch is as follows:\n",
    "- $l(W,X)=\\frac{1}{batch\\_size}\\sum_{i=1}^{batch\\_size}l(b^{i},y^{i};W)=-\\frac{1}{batch\\_size}\\sum_{i=1}^{batch\\_size}\\left[y^{i}\\log p^{i}+(1-y^{i})\\log(1-p^{i})\\right]$ where $p^{i} = p(y^i=1 \\mid b^i) = sigmoid(z^i)= \\frac{1}{1+e^{-z^i}}= \\frac{1}{1+e^{-b^iW}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\"> Backward propagation </span>\n",
    "\n",
    "We now compute the gradient of the loss incurred by a mini-batch $l(W,X)$ w.r.t the model parameter $W$. This can be done conveniently via backward propagation with some matrix multiplications. The mathematical tool for this derivation is the chain rule based on the computational process of the forward propagation.\n",
    "- $z = XW$ --> $p = sigmoid(z)$ --> $l = - mean(y*log(p) + (1-y)*log(1-p))$ where $*$ is element-wise product between two vectors.\n",
    "\n",
    "We derive as follows:\n",
    " - $ \\frac{\\partial l}{\\partial W}\t=\\frac{1}{batch\\_size}\\sum_{i=1}^{batch\\_size}\\frac{\\partial l(b_{i},y_{i};W)}{\\partial W}=\\frac{1}{batch\\_size}\\sum_{i=1}^{batch\\_size}\\frac{\\partial l(b_{i},y_{i};W)}{\\partial p^{i}}\\frac{\\partial p^{i}}{\\partial z^{i}}\\frac{\\partial z^{i}}{\\partial W}\n",
    "\t=\\frac{1}{batch\\_size}\\sum_{i=1}^{batch\\_size}\\left[\\frac{-y^{i}}{p^{i}}+\\frac{1-y^{i}}{1-p^{i}}\\right]p^{i}\\left(1-p^{i}\\right)b^{i}$\n",
    "\n",
    "To facilitate the computation, we can rewrite the above derivative in the form of matrix multiplication as follows:\n",
    "- $\\frac{\\partial l}{\\partial W}=mean\\left(X^{T}\\left[\\left(\\frac{-y}{p}+\\frac{1-y}{1-p}\\right)*p*(1-p)\\right]\\right)\\in\\mathbb{R}^{2\\times1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\"> SGD update </span>\n",
    "\n",
    "We update the model based on the gradient of the mini-batch w.r.t $W$ as:\n",
    "- $W = W - \\eta*\\frac{\\partial l}{\\partial W}$ where $\\eta >0$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> I.2 Implementation of logistic regression </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:#0b486b\"> First, we create a simple synthetic dataset for logistic regressions </span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to generate a few data points on a two-dimensional axis and assume that the optimal coefficient of the logistic regression is known to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [2.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp/ipykernel_36840/4221109522.py:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  W_opt = np.array([[1],[2]], dtype=np.float)    #the optimal W\n"
     ]
    }
   ],
   "source": [
    "W_opt = np.array([[1],[2]], dtype=np.float)    #the optimal W\n",
    "print(W_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the training set of 2D data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (500, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomly generate 500 synthetic data points\n",
    "N =500\n",
    "delta = 2\n",
    "X1 = (np.random.rand(N) -0.5)*2*delta\n",
    "X2 = (np.random.rand(N) -0.5)*2*delta\n",
    "X_train = np.array([x for x in zip(X1,X2)])\n",
    "print(\"X_train shape: {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign labels (i.e., $0$ or $1$) for the data points according to if they stay on what side of the optimal hyperplane (W_opt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive labels: 252, number of negative labels: 248\n"
     ]
    }
   ],
   "source": [
    "# Generate the labels for those data points\n",
    "y_value = X_train.dot(W_opt) + np.random.normal(0.0, 0.2)\n",
    "y_train = np.ones(N).reshape([N,1])\n",
    "y_train[np.where(y_value <0)]= 0\n",
    "num_pos = len(np.where(y_train==1)[0])\n",
    "num_neg = len(np.where(y_train==0)[0])\n",
    "print(\"Number of positive labels: {}, number of negative labels: {}\".format(num_pos, num_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the data points with labels and the optimal decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code visulizes the hyperplane, i.e., the decision boundary, which is a line in the two-dimensional case\n",
    "def visualize_hyperplane(X= None, y= None, W=None):\n",
    "    X_pos = X[np.where(y ==1)[0]]\n",
    "    X_neg = X[np.where(y ==0)[0]]\n",
    "    plt.scatter(X_pos[:,0], X_pos[:,1], label='1',  c='green', marker='+')\n",
    "    plt.scatter(X_neg[:,0], X_neg[:,1], label='0', c='blue', marker='_')\n",
    "    plt.legend(loc='upper right')\n",
    "    f = lambda x1: -W[0,0]*x1/W[1,0] #Plot the decision boundary\n",
    "    X1 = np.linspace(-2,2,500)\n",
    "    plt.plot(X1, f(X1),  'r--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "visualize_hyperplane(X_train, y_train, W_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:#0b486b\"> Now we define the logistic regression model </span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "# Visualize the hpyterplane and the training loss\n",
    "def visualize_hyperplane_loss(X= None, y= None, W= None, losses= None): \n",
    "    X_pos = X[np.where(y ==1)[0]]\n",
    "    X_neg = X[np.where(y ==0)[0]]\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(X_pos[:,0], X_pos[:,1], label='1',  c='green', marker='+')\n",
    "    plt.scatter(X_neg[:,0], X_neg[:,1], label='0', c='blue', marker='_')\n",
    "    plt.legend(loc='upper right')\n",
    "    f = lambda x1: -W[0,0]*x1/W[1,0] #Plot the decision boundary\n",
    "    X1 = np.linspace(-2,2,500)\n",
    "    plt.plot(X1, f(X1),  'r--')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(losses, label = 'Epoch loss', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1.0/ (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for forward propagation w.r.t the mini-batch $X$ or the set of data points $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model and the corresponding loss, which is similar to foward pass in deep neural networks\n",
    "def forward(X= None, y=None, W= None, eps= 1E-10):\n",
    "    z = X.dot(W)\n",
    "    p = sigmoid(z)\n",
    "    losses = [-np.log(p[i]+eps) if y[i]==1 else -np.log(1- p[i]+eps) for i in range(len(y))] \n",
    "    loss = np.mean(losses)\n",
    "    return (z,p,loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for backward propagation in which we compute the gradient of the loss of $X$ w.r.t $W$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradient of the loss by the chain rule, which is similar to backpropagate in deep neural networks\n",
    "def grad(X= None, y= None, z= None, p=None, eps= 1E-10):\n",
    "    batch_size = len(y)\n",
    "    grad_p = [-1.0/(p[i] + eps) if y[i]==1 else 1.0/(1-p[i]+ eps) for i in range(batch_size)]\n",
    "    grad_p = np.array(grad_p).reshape([batch_size,1])\n",
    "    grad_z = grad_p*p*(1-p)\n",
    "    grad_W = X.transpose().dot(grad_z)*(1.0/batch_size)  #refer to the formula to compute the gradient in Section I\n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:#0b486b\"> Here we train our logistic regression model on the synthetic dataset with gradient descent</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider three schemes:\n",
    "- (i) Gradient descent in which we use the entire dataset (i.e., $X$ is the entire dataset) to compute the gradient.\n",
    "- (ii) SGD in which we use mini-batches to compute gradient for which the mini-batches are uniformly sampled from the training set at each iteration.\n",
    "- (iii) SGD in which we use mini-batches to compute gradient, but we first shuffle the training set, split the training set into many equal mini-batches (folds) with the same number of data points (i.e., $batch\\_size$), and in each iteration we use a mini-batch to feed to the network. Although this mini-batch generating strategy is a bit diverge from the theory of SGD, it always obtains comparable performance compared to (ii), hence widely being used in training deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for GD in (i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x000001D32EDBA8D0>\n",
      "Epoch 1: loss =0.7154882969291263\n",
      "Epoch 2: loss =0.5989421386552528\n",
      "Epoch 3: loss =0.5135954447989376\n",
      "Epoch 4: loss =0.45049102631649235\n",
      "Epoch 5: loss =0.40297217404027946\n",
      "Epoch 6: loss =0.3664499708711034\n",
      "Epoch 7: loss =0.33781519699878565\n",
      "Epoch 8: loss =0.31494482541234575\n",
      "Epoch 9: loss =0.29636458660758513\n",
      "Epoch 10: loss =0.2810311945323349\n",
      "Epoch 11: loss =0.26819284614719646\n",
      "Epoch 12: loss =0.25729877999747147\n",
      "Epoch 13: loss =0.2479395166789568\n",
      "Epoch 14: loss =0.2398065577740922\n",
      "Epoch 15: loss =0.2326646732026439\n",
      "Epoch 16: loss =0.2263325061720995\n",
      "Epoch 17: loss =0.2206687857525645\n",
      "Epoch 18: loss =0.21556238999898858\n",
      "Epoch 19: loss =0.2109250965422451\n",
      "Epoch 20: loss =0.20668623601737743\n",
      "Epoch 21: loss =0.20278870994756015\n",
      "Epoch 22: loss =0.19918599808463705\n",
      "Epoch 23: loss =0.1958398905274089\n",
      "Epoch 24: loss =0.19271875560237356\n",
      "Epoch 25: loss =0.1897962071078892\n",
      "Epoch 26: loss =0.18705007156268735\n",
      "Epoch 27: loss =0.18446158246010955\n",
      "Epoch 28: loss =0.18201474747368512\n",
      "Epoch 29: loss =0.179695848294956\n",
      "Epoch 30: loss =0.17749304282368336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IoffContext at 0x1d32f98d250>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib\n",
    "W = np.array([[-1],[1]])\n",
    "lst_W1 = [copy.copy(W)]\n",
    "epochs =30\n",
    "eta = 0.5\n",
    "plt.ion()\n",
    "losses =[]\n",
    "for epoch in range(epochs):\n",
    "    # We update the coefficients through all the data points\n",
    "    visualize_hyperplane_loss(X_train, y_train, W, losses)\n",
    "    plt.pause(1)\n",
    "    plt.draw()\n",
    "    X = X_train\n",
    "    y= y_train\n",
    "    z, p, loss= forward(X,y, W)\n",
    "    losses.append(loss)\n",
    "    # Compute gradient\n",
    "    grad_W = grad(X,y,z,p)\n",
    "    # Update coefficients with gradient descent\n",
    "    W = W - eta*grad_W\n",
    "    lst_W1.append(copy.copy(W))\n",
    "    print(\"Epoch {}: loss ={}\".format(epoch +1, loss))\n",
    "    if(epoch< epochs-1):\n",
    "        plt.clf()\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for SGD with the strategy in (ii)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Epoch 1: loss=0.28263830522177735\n",
      "Epoch 2: loss=0.14383304169656974\n",
      "Epoch 3: loss=0.1313595930380778\n",
      "Epoch 4: loss=0.1111207301924904\n",
      "Epoch 5: loss=0.10211814777376738\n",
      "Epoch 6: loss=0.10678510928725903\n",
      "Epoch 7: loss=0.09359783634744148\n",
      "Epoch 8: loss=0.08014201151866093\n",
      "Epoch 9: loss=0.08501858264749151\n",
      "Epoch 10: loss=0.09023683219863451\n",
      "Epoch 11: loss=0.08981034492034393\n",
      "Epoch 12: loss=0.07651915979498083\n",
      "Epoch 13: loss=0.06681117911935149\n",
      "Epoch 14: loss=0.06805047641936708\n",
      "Epoch 15: loss=0.07356461782603947\n",
      "Epoch 16: loss=0.0690323052818154\n",
      "Epoch 17: loss=0.059373069797059704\n",
      "Epoch 18: loss=0.06003848089598515\n",
      "Epoch 19: loss=0.05936334545692629\n",
      "Epoch 20: loss=0.06992935908353139\n",
      "Epoch 21: loss=0.05622373524757706\n",
      "Epoch 22: loss=0.06069430949646166\n",
      "Epoch 23: loss=0.05759694161614641\n",
      "Epoch 24: loss=0.04705017041772959\n",
      "Epoch 25: loss=0.05246440591777977\n",
      "Epoch 26: loss=0.054504632346524265\n",
      "Epoch 27: loss=0.053002617370183726\n",
      "Epoch 28: loss=0.059161282574260066\n",
      "Epoch 29: loss=0.06342839735244316\n",
      "Epoch 30: loss=0.05651831567250155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IoffContext at 0x1d3322164c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib\n",
    "plt.ion()\n",
    "\n",
    "W = np.array([[-1],[1]])\n",
    "lst_W2 = [copy.copy(W)]\n",
    "epochs =30\n",
    "eta = 0.5\n",
    "batch_size = 16\n",
    "iter_per_epoch = int(N/batch_size)\n",
    "epoch_losses= []\n",
    "for epoch in range(epochs):\n",
    "    losses =[]\n",
    "    # We update the coefficients in each random batch\n",
    "    for i in range(iter_per_epoch):\n",
    "        visualize_hyperplane_loss(X_train, y_train, W, epoch_losses)\n",
    "        plt.pause(0.02)\n",
    "        # randomly sample a batch of data points\n",
    "        idxs= np.random.choice(np.arange(N), batch_size, replace= False)\n",
    "        X= X_train[idxs]\n",
    "        y = y_train[idxs]\n",
    "        z, p, batch_loss= forward(X, y,W)\n",
    "        losses.append(batch_loss)\n",
    "        grad_W = grad(X,y,z,p)\n",
    "        W = W - eta*grad_W\n",
    "        if((epoch == epochs-1) and (i== iter_per_epoch-1)):\n",
    "            pass\n",
    "        else:\n",
    "            plt.clf()\n",
    "    lst_W2.append(copy.copy(W))\n",
    "    print(\"Epoch {}: loss={}\".format(epoch+1, np.mean(losses)))\n",
    "    epoch_losses.append(np.mean(losses))\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for SGD with the strategy in (iii)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "\n",
    "plt.ion()\n",
    "W = np.array([[-1],[1]])\n",
    "lst_W3 = [copy.copy(W)]\n",
    "epochs =30\n",
    "eta = 0.5\n",
    "batch_size = 16 \n",
    "iter_per_epoch = int(N/batch_size)\n",
    "epoch_losses =[]\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    # We update the coefficients in each sequential batch\n",
    "    for idx_start in range(0, N, batch_size):\n",
    "        visualize_hyperplane_loss(X_train, y_train, W, epoch_losses)\n",
    "        plt.pause(0.05)\n",
    "        # We go through the batches sequencially, i.e., non-randomly\n",
    "        idx_end = min(N, idx_start + batch_size)\n",
    "        X= X_train[idx_start:idx_end]\n",
    "        y= y_train[idx_start:idx_end]\n",
    "        z, p, batch_loss= forward(X, y, W)\n",
    "        losses.append(batch_loss)\n",
    "        grad_W = grad(X, y, z,p)\n",
    "        W = W - eta*grad_W\n",
    "        if((epoch==epochs-1) and (idx_end==N)):\n",
    "            pass\n",
    "        else:\n",
    "            plt.clf()\n",
    "    lst_W3.append(copy.copy(W))\n",
    "    print(\"Epoch {}: loss={}\".format(epoch+1, np.mean(losses)))\n",
    "    epoch_losses.append(np.mean(losses))\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the trajectories of the models for three strategies (i), (ii), and (iii)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "\n",
    "def visualize_W(lst_W1= None, lst_W2= None, lst_W3= None, k=1, W_opt= None):\n",
    "    plt.plot([lst_W1[i][0] for i in range(k)],[lst_W1[i][1] for i in range(k)], \"--s\", markersize= 3, label='GD', color='green')\n",
    "    plt.plot([lst_W2[i][0] for i in range(k)],[lst_W2[i][1] for i in range(k)], \"--o\", markersize= 3, label='SGD 1', color='blue')\n",
    "    plt.plot([lst_W3[i][0] for i in range(k)],[lst_W3[i][1] for i in range(k)], \"-->\", markersize= 3, label='SGD 2', color='orange')\n",
    "    plt.plot([W_opt[0,0]],[W_opt[1,0]], \"*\", markersize= 15, color= 'red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "\n",
    "plt.ion()\n",
    "for i in range(len(lst_W1)):\n",
    "    visualize_W(lst_W1, lst_W2, lst_W3, i+1, W_opt)\n",
    "    if i == 0:\n",
    "        plt.legend(loc=\"upper left\")\n",
    "    plt.pause(2)\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion of the comparison**\n",
    "\n",
    "With the current learning rate, although the performances are comparable, in terms of the convergence to the optimal solution, GD really converges to the optimal solution $W\\_opt$, while SGD with the strategies (ii) and (iii) do not converge to the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 1</span>**: Try with smaller learning rates and different initial $W$ to compare the models obtained by three strategies in (i), (ii), and (iii). Report your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 2</span>**: Implement SGD with momentum (refer [here](https://ruder.io/optimizing-gradient-descent/)) and compare with the standard SGD in terms of the convergence rate to the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\"> II. Conduct a DNN model on the MNIST dataset with SGD</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:#0b486b\"> II.1 We first load the MNIST dataset in TensorFlow</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train_full_img, y_train_full), (X_test_img, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "num_train = X_train_full_img.shape[0]\n",
    "num_test = X_test_img.shape[0]\n",
    "X_train_full = X_train_full_img.reshape(num_train,-1)/255.0\n",
    "X_test = X_test_img.reshape(num_test, -1)/255.0\n",
    "print(X_train_full.shape, y_train_full.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:#0b486b\"> II.2 We build a deep neural network with several fully-connected layers</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self, n_classes= 10, optimizer= tf.keras.optimizers.SGD(learning_rate= 0.001), batch_size= 32):\n",
    "        self.n_classes= 10\n",
    "        self.batch_size= batch_size\n",
    "        self.optimizer = optimizer\n",
    "        #create a tensorflow dataset\n",
    "        self.train_full_set = tf.data.Dataset.from_tensor_slices((X_train_full, y_train_full)).shuffle(1000)\n",
    "        #take train and valid sets from full dataset\n",
    "        self.train_set = self.train_full_set.take(50000)\n",
    "        self.valid_set = self.train_full_set.skip(50000).take(10000)\n",
    "        #batching train and valid sets\n",
    "        self.train_set= self.train_set.batch(self.batch_size).prefetch(1)\n",
    "        self.valid_set= self.valid_set.batch(self.batch_size).prefetch(1)\n",
    "        \n",
    "    def build(self):\n",
    "        self.model= Sequential([Dense(20, activation='relu'), Dense(20, activation= 'relu'),\n",
    "                               Dense(self.n_classes, activation= 'softmax')])\n",
    "        \n",
    "    def compute_loss(self, X, y): #X is data batch, y is label batch\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(X,y)\n",
    "    \n",
    "    def compute_grads(self, X, y):\n",
    "        with tf.GradientTape() as g:  #use gradient tape to compute gradients\n",
    "            loss= self.compute_loss(X,y)\n",
    "        grads= g.gradient(loss, self.model.trainable_variables) #compute gradients w.r.t. all trainable variables\n",
    "        return grads\n",
    "    \n",
    "    def train_one_batch(self, X, y): #train in one batch\n",
    "        grads= self.compute_grads(X,y)\n",
    "        #the gradients will be applied according to optimizer for example SGD, Adam, and etc.\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "    \n",
    "    def train(self, epochs= 20, batch_size):\n",
    "        batch_per_epoch = tf.data.experimental.cardinality(self.train_set).numpy() #number of batches in train dataset\n",
    "        for epoch in range(epochs):\n",
    "            for (batch_index, batch_data) in zip(range(batch_per_epoch), self.train_set):\n",
    "                \n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(tf.keras.Model):\n",
    "    def __init__(self, num_classes= 10):\n",
    "        super(DNN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.dense1 = tf.keras.layers.Dense(20, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(20, activation= 'relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(self.num_classes, activation= 'softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        h = self.dense1(inputs)\n",
    "        h = self.dense2(h)\n",
    "        h = self.dense3(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = DNN(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn.build(input_shape=(None, 784))\n",
    "dnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:#0b486b\"> II.3 Next we train the DNN on MNIST by SGD</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "dnn.compile(optimizer= opt, loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance on the test data\n",
    "loss, test_acc = dnn.evaluate(X_test, y_test)\n",
    "print('Test acc= {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:#0b486b\"> II.4 We tune the learning rate using the validation data set</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_learning_rates = [0.1, 0.01, 0.001]\n",
    "best_acc= - np.inf\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "for lr in lst_learning_rates:\n",
    "    dnn = DNN(num_classes=10)\n",
    "    dnn.build(input_shape= (None, 784))\n",
    "    opt.learning_rate = lr\n",
    "    dnn.compile(optimizer= opt, loss= 'sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
    "    print(\"Training with optimizer= {}, learning rate= {}\".format(opt.get_config()['name'], lr))\n",
    "    dnn.fit(X_train, y_train, batch_size= 32, epochs=10, verbose=0)\n",
    "    valid_loss, valid_acc = dnn.evaluate(X_valid, y_valid)\n",
    "    print('----> valid acc={}, valid loss= {}'.format(valid_acc, valid_loss))\n",
    "    if(valid_acc > best_acc):\n",
    "        best_acc= valid_acc\n",
    "        best_model = dnn\n",
    "        best_opt = opt\n",
    "        best_lr = lr\n",
    "\n",
    "print('\\nThe best model is with optimizer= {}, learning rate= {}'.format(best_opt.get_config()['name'], best_lr))\n",
    "print('Evaluating the best model on the test set')\n",
    "_, test_acc= best_model.evaluate(X_test, y_test)\n",
    "print('Test acc= {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 3</span>**: Develop a feed-forward neural network to work with the MNIST dataset according to the following requirements:\n",
    "- The architecture is $Input \\rightarrow 20 (ReLU) \\rightarrow 40 (ReLU) \\rightarrow 20 (ReLU) \\rightarrow  Output (softmax)$.\n",
    "- Extend the code to allow doing grid search for tunining the learning rate in the list $[0.1, 0.01, 0.001]$ and optimizer in the list $[tf.keras.optimizers.Adam(), tf.keras.optimizers.RMSprop(), tf.keras.optimizers.SGD(momentum=0.2)]$ and save the best model to the hard disk ([link](https://www.tensorflow.org/guide/keras/save_and_serialize)). Plot the training progress of the best model (e.g., training and valid accuracies, training and valid losses).\n",
    "- Load the best model and evaluate on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf2_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "92c79073133677da89801d1b4bc42714b1a3d83cbc90a7f9c522b094b613b522"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
