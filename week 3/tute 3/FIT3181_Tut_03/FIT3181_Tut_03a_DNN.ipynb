{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (2022)</span>\n",
    "***\n",
    "*CE/Lecturer (Clayton):*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Lecturer (Malaysia):*  **Dr Lim Chern Hong** | lim.chernhong@monash.edu <br/>  <br/>\n",
    "*Tutor:*  **Mr Thanh Nguyen** \\[Thanh.Nguyen4@monash.edu \\] |**Mr Tuan Nguyen**  \\[tuan.ng@monash.edu \\] |**Mr Anh Bui** \\[tuananh.bui@monash.edu\\] | **Dr Binh Nguyen** \\[binh.nguyen1@monash.edu \\] | **Mr Md Mohaimenuzzaman** \\[md.mohaimen@monash.edu \\] |**Mr James Tong** \\[james.tong1@monash.edu \\]\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Tutorial 3a: Feed-forward Neural Nets with TensorFlow 1.x</span> \n",
    "**The purpose of this tutorial is to demonstrate how to work with an open source software library for developing deep neural networks apllications, called TensorFlow. In this tutorial, we will focus on**:  \n",
    "- ***Inspect the common pipeline of deep learning*.**\n",
    "- ***How to implement a feedforward neural net for a multi-class classfication problem using TF 1.x in Tutorial 3a (this tutorial)*.**\n",
    "- ***How to implement a feedforward neural net for a multi-class classfication problem using TF 2.x in Tutorial 3b*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> II.1 Feedforward Neural Network </span> <span style=\"color:red\">***** (highly important)</span>\n",
    "#### <span style=\"color:#0b486b\"> Tutorial objective </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will consider a fairly realistic deep NNs with *three* layers plus the *output* layer. Its architecture will be specified as: $16 \\rightarrow 10 (ReLU) \\rightarrow 20 (ReLU) \\rightarrow 15 (ReLu) \\rightarrow 26$. This means:\n",
    "- Input size is 16\n",
    "- First layer has 10 hidden units with ReLU activation functions\n",
    "- Second layer has 20 hidden units with 20 ReLU activiation functions\n",
    "- Third layer has 15 hidden units with 15 ReLU activiation functions\n",
    "- And output layer is logit layer with 26 hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network, for example, can take the `letter` dataset input with $16$ features and with $26$ classes (A-Z). **Our objective in this tutorial is to implement this specific network in `TensorFlow 1.x`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Specifying the Neural Network Architecture </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this network as in the figure below. Please note that for readability, the number of hidden units in the figure might not correspond exactly to the actual size of the hidden units used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DNN_Pipeline.PNG\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the above figure shows the pipeline of the entire process for feeding a mini-batch of batch size $32$ into the network. Using ***mini-batch*** is a common way to train deep NNs in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us denote the mini-batch by $X_b= \\{(x_1, y_1),\\dots, (x_{32}, y_{32})\\}$. The mini-batch can be stored using a $2D$ tensor with the shape $(32, 16)$. Assume that in this network, we use the activation function $ReLu$ where $ReLu(t)= \\max\\{0, t\\}$. The computation in the forward propagation step is as follows:\n",
    "- Input $X_b$ with mini-batch size of 32\n",
    "- $h_1= ReLu(X_b \\times W^1 + b^1)\\in \\mathbb{R}^{32 \\times 10}$. \n",
    "- $h_2= ReLu(h_1 \\times W^2 + b^2\\in \\mathbb{R}^{32 \\times 20}$. \n",
    "- $h_3= ReLu(h_2 \\times W^3 + b^3\\in \\mathbb{R}^{32 \\times 15}$. \n",
    "- $logits= h_3 \\times W^4 + b^4 \\in \\mathbb{R}^{32 \\times 26}$\n",
    "- $p = softmax(logits) \\in \\mathbb{R}^{32 \\times 26}$ <br>\n",
    "\n",
    "where we note that the activation function is perfomed element-wise and the softmax function is used to transform a vector of scalars to a discrete distribution as: \n",
    "\n",
    "$$softmax(z)=\\big[\\frac{\\exp(z_i)}{\\sum_{j=1}^{26}{\\exp(z_j)}}\\big]_{i=1}^{26}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "The $k$-th row $p_k$ of the matrix $p$ can represent the probability distribution to classify the data point $x_k$ to the classes $1,2,\\dots,26$. In particular, we have:\n",
    "\n",
    "$$p_{km}= p(y_k=m \\mid x_k)  \\text{ for }  m=1,2,\\dots,26$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\"> Exercise 1</span>** : Explain why the dimension for $h_1$ is $32\\times 10$? Similarly, please work out the dimension for $h2, h3, logits$ and $p$.\n",
    "\n",
    "ME: \n",
    "- h1 = 32 x 10\n",
    "- h2 = (32x10) * ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Specifying the Loss Function </span>\n",
    "Essiential to training a deep NN is the concept of the **loss function**. This function will tell us how good the network is predicting, and hence we can use this loss to find the network weights in such a way that the loss can be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification task, a common approach is to use the **cross-entropy** loss function. Given a data-label instance $(x_k,y_k)$ where feature $x_k\\in \\mathbb{R}^{16}$ and the label $y_k \\in \\{1,2,...,26\\}$ is a numeric label (for example if $x_k$ is in the class 2, then $y_k =2 $ and its one-hot vector $1_{y_k}=[0,1,0,...,0]$). The cross-entroty between the classification distribution $p_k$ returned from the NN and true label distribution $y_k$ is defined as:\n",
    "$$cross\\_entropy(1_{y_k}, p_k)=-\\sum_{j=1}^{26}y_{kj}\\log{p_{kj}}=-\\log p_{k,y_k}$$. This loss basically enforces the model to predict the label as close as the true label by minimizing $cross\\_entropy(1_{y_k}, p_k)$.\n",
    "\n",
    "The above loss function was applied for each instance. For the entire current mini-batch, our loss function becomes: \n",
    "$$\\min \\sum_{k=1}^{32}cross\\_entropy(1_{y_k}, p_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\"> Exercise 2: </span>** : **<span style=\"color:#0b486b\">In the corss-entropy equation above, $y_k$ is the class for $x_k$, explain why the end result is $-\\log p_{k,y_k}$.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\"> Exercise 3: </span>** : **<span style=\"color:#0b486b\">Let $p=[0.1, 0.3, 0.6]$ and $q=[0.0, 0.5, 0.5]$ be two discrete distributions, what is the $cross\\_entropy(q,p)$ ?</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> II.2 Implementation with TensorFlow 1.x</span> <span style=\"color:red\">**** (important)</span>\n",
    "We now shall implement the aforementioned network with the architecture of $16 \\rightarrow 10 (ReLU) \\rightarrow 20 (ReLU) \\rightarrow 15 (ReLu) \\rightarrow 26$ in Tensorflow using the dataset `letter`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This letter dataset can be found at [the LIBSVM website](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#letter). Here is the dataset information:\n",
    "-  *The objective is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical pipeline process of implementing a deep learning model is as follows:\n",
    "\n",
    "1. **Data processing**: \n",
    "   - Load the dataset and split into train, valid, and test sets.  \n",
    "     \n",
    "2. **Construction phase**: \n",
    "   - Define the NN model and construct the corresponding computational graph.\n",
    "   - Define the loss function and the relevant measures of performance of interest (accuracy, F1, and AUC).\n",
    "    \n",
    "3. **Execution and evaluation phase**: \n",
    "   - Train the model using mini-batches from the train set by minimizing the loss function with an optimizer.\n",
    "   - Predict on the test set and access its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">1. Data Processing </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `sklearn` to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X data shape: (15000, 16)\n",
      "y data shape: (15000, 1)\n",
      "# classes: 26\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      " 19. 20. 21. 22. 23. 24. 25. 26.]\n"
     ]
    }
   ],
   "source": [
    "data_file_name= \"letter_scale.libsvm\"\n",
    "data_file = os.path.abspath(\"./Data/\" + data_file_name)\n",
    "X_data, y_data = load_svmlight_file(data_file)\n",
    "X_data= X_data.toarray()\n",
    "y_data= y_data.reshape(y_data.shape[0],-1)\n",
    "print(\"X data shape: {}\".format(X_data.shape))\n",
    "print(\"y data shape: {}\".format(y_data.shape))\n",
    "print(\"# classes: {}\".format(len(np.unique(y_data))))\n",
    "print(np.unique(y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `sklearn` to split the dataset into the train, validation, and test sets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "# split function for train,test and validation\n",
    "def train_valid_test_split(data, target, train_size, test_size):\n",
    "    valid_size = 1 - (train_size + test_size)\n",
    "    X1, X_test, y1, y_test = train_test_split(data, target, test_size = test_size, random_state= 33)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X1, y1, test_size = float(valid_size)/(valid_size+ train_size))\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would like to encode the label in the form of numeric vector. For example, we want to turn $y\\_data=[\"cat\", \"dog\", \"cat\", \"lion\", \"dog\"]$ to $y\\_data=[0,1,0,2,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, in the following segment of code, we use the object `le` as an instance of the class `preprocessing.LabelEncoder()` which supports us to transform catefgorial labels in `y_data` to numerical vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 15 18  7  7  5 13 17 12  3 21  0 10  3 18  7  4 25 14 16]\n",
      "[25 15 18  7  7  5 13 17 12  3 21  0 10  3 18  7  4 25 14 16]\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(y_data)\n",
    "y_data= le.transform(y_data)\n",
    "print(y_data[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the function defined above to prepare our data for training, validating and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 16) (1500, 16) (1500, 16)\n",
      "(12000,) (1500,) (1500,)\n",
      "lables: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test = train_valid_test_split(X_data, y_data, train_size=0.8, test_size=0.1)\n",
    "y_train= y_train.reshape(-1)\n",
    "y_test= y_test.reshape(-1)\n",
    "y_valid= y_valid.reshape(-1)\n",
    "print(X_train.shape, X_valid.shape, X_test.shape)\n",
    "print(y_train.shape, y_valid.shape, y_test.shape)\n",
    "print(\"lables: {}\".format(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We catch some information of the training set which will be reused later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size= int(X_train.shape[0])\n",
    "n_features= int(X_train.shape[1])\n",
    "n_classes= len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, in real-world implementation of deep learni|ng models, we use Stochastic Gradient Descent (SGD). Input to this algorithm is a sequence of **mini-batch** of data drawn from the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">2. Construction Phase </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build up a feedforward neural network with the architecture: $16 \\rightarrow 10 (ReLU) \\rightarrow 20 (ReLU) \\rightarrow 15 (ReLu) \\rightarrow 26$ in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in= n_features    # dimension of input\n",
    "n1= 10              # number of hidden units at the first layer\n",
    "n2= 20              # number of hidden units at the second layer\n",
    "n3= 15              # number of hidden units at the third layer\n",
    "n_out= n_classes    # number of classification classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `dense_layer` represents a fully connected layer in a deep learning network. This takes $W,b$ and input as inputs and returns $\\sigma(W \\times input + b)$ where the activation function $sigma$ is specified by the parameter `act`.\n",
    "- In `TensorFlow`, we can refer to the `activation functions` as `tf.nn.relu`, `tf.nn.sigmoid`, `tf.nn.tanh`, and etc.\n",
    "- You can also define your own activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ME: we dont need activation function when it is regression\n",
    "def dense_layer(inputs, output_size, act=None, name=\"hidden-layer\"):\n",
    "    with tf.name_scope(name):\n",
    "        input_size= int(inputs.get_shape()[1])\n",
    "        W_init = tf.random.normal([input_size, output_size], mean=0, stddev= 0.1, dtype= tf.float32)\n",
    "        b_init= tf.random.normal([output_size], mean=0, stddev= 0.1, dtype= tf.float32)\n",
    "        W= tf.Variable(W_init, name= \"W\")\n",
    "        b= tf.Variable(b_init, name=\"b\")\n",
    "        Wxb= tf.matmul(inputs, W) + b\n",
    "        if act is None:\n",
    "            return Wxb\n",
    "        else:\n",
    "            return act(Wxb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now construct the computational graph. But before that we need to reset the default graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the cross-entropy loss. Note that in TensorFlow you can use two of following functions for evaluating the cross-entropy loss:\n",
    "- `tf.nn.sparse_softmax_cross_entropy_with_logits`: if the labels `y_train` is in the categorial format (e.g., `y_train=[0,1,0,1,1,2]`).\n",
    "- `tf.nn.softmax_cross_entropy_with_logits`: if the labels `y_train` is in the one-hot format (e.g., `y_train=[[1,0,0], [0,1,0], [1,0,0], [0,0,1]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.name_scope(\"network\"):\n",
    "    X= tf.placeholder(shape=[None, n_in], dtype= tf.float32)\n",
    "    y= tf.placeholder(shape=[None], dtype= tf.int32)\n",
    "    # input is the first parameter in dense_layer() and many more\n",
    "    h1= dense_layer(X, n1, act= tf.nn.relu, name= \"layer1\")\n",
    "    h2= dense_layer(h1, n2, act= tf.nn.relu, name= \"layer2\")\n",
    "    h3= dense_layer(h2, n3, act= tf.nn.relu, name= \"layer3\")\n",
    "    logits= dense_layer(h3, n_out, name=\"logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to specify an optimizer to minimize the loss. Here, we are using the Adam optimizer for this optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    # ME: we are using numeric label instead of one-hot vectors\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \n",
    "                                                              logits=logits, \n",
    "                                                              name='xentropy')\n",
    "    loss= tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    tf.summary.scalar(\"loss\", loss)    #summarize the loss\n",
    "    optimizer= tf.train.AdamOptimizer(learning_rate=0.001)    # use optimizer with the learning rate\n",
    "    train_op= optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we add the line ***tf.summary.scalar(\"loss\", loss)*** to add to the summary the loss.\n",
    "\n",
    "We also wish to estimate the accuracy of our model. For this you can use the [*`in_top_k()`* function](https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k) with *k=1*. This returns a 1D tensor full of boolean values, so we need to cast these booleans to floats and then compute the average. This will give us the networkâ€™s overall accuracy. We insert the line ***tf.summary.scalar(\"accuracy\", accuracy)*** to add to the summary the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('evaluation'):\n",
    "    # why use top_k ? ANS: we want to get the top k probabilities in the matrix\n",
    "    # after we have model and perform backpropagation, \n",
    "    # we have a matrix of probabilities after performing softmax\n",
    "    # we might want to get the top k score, instead of getting the top-most score\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)  #summarize the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define two FileWriters to write the summary to two log folders. By this way, we can plot the train, valid losses (or accuracies) on the same graph. Note that you can use this trick when you want to display some plots on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.exists(\"./logs/train\")):\n",
    "    os.makedirs(\"./logs/train\")\n",
    "\n",
    "if(not os.path.exists(\"./logs/val\")):\n",
    "    os.makedirs(\"./logs/val\")\n",
    "\n",
    "merged= tf.summary.merge_all()\n",
    "train_writer= tf.summary.FileWriter(\"./logs/train\")\n",
    "valid_writer= tf.summary.FileWriter(\"./logs/val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">3. Execution and Evaluation Phase </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `execution phase`, we need to create a `TensorFlow session`, then initialize `all variables` in the graph, execute `train_op`, and query the values of necessary nodes (e.g., `loss` and `accuracy`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize all variables\n",
    "  - `init= tf.global_variables_initializer()` and `sess.run(init)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Execute `train_op` when feeding mini-batches to the network\n",
    "  - `sess.run([train_op], feed_dict={X:x_batch, y:y_batch})`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query the values of necessary nodes\n",
    "  - `val_loss, val_accuracy= sess.run([loss, accuracy], feed_dict={X:X_valid, y:y_valid})`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as a rule of machine learning, during training phase, we **cannot** touch the `test set` and only use this set when we need to output the predictive performance of a trained model.\n",
    "-  Output the predictive performance on the test set\n",
    "   - `test_accuracy= sess.run(accuracy, feed_dict={X:X_test, y:y_test})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: valid loss=2.4827, valid acc=0.2527\n",
      "########: train loss=2.4520, train acc=0.2345\n",
      "Epoch 2: valid loss=1.9994, valid acc=0.3760\n",
      "########: train loss=1.9561, train acc=0.3977\n",
      "Epoch 3: valid loss=1.7644, valid acc=0.4653\n",
      "########: train loss=1.7070, train acc=0.4885\n",
      "Epoch 4: valid loss=1.6883, valid acc=0.4967\n",
      "########: train loss=1.6146, train acc=0.5151\n",
      "Epoch 5: valid loss=1.6450, valid acc=0.5120\n",
      "########: train loss=1.5655, train acc=0.5250\n",
      "Epoch 6: valid loss=1.6020, valid acc=0.5247\n",
      "########: train loss=1.5202, train acc=0.5391\n",
      "Epoch 7: valid loss=1.5615, valid acc=0.5320\n",
      "########: train loss=1.4789, train acc=0.5489\n",
      "Epoch 8: valid loss=1.5221, valid acc=0.5467\n",
      "########: train loss=1.4395, train acc=0.5628\n",
      "Epoch 9: valid loss=1.4845, valid acc=0.5607\n",
      "########: train loss=1.4021, train acc=0.5778\n",
      "Epoch 10: valid loss=1.4448, valid acc=0.5760\n",
      "########: train loss=1.3628, train acc=0.5906\n",
      "Epoch 11: valid loss=1.4079, valid acc=0.5800\n",
      "########: train loss=1.3218, train acc=0.6043\n",
      "Epoch 12: valid loss=1.3759, valid acc=0.5927\n",
      "########: train loss=1.2857, train acc=0.6147\n",
      "Epoch 13: valid loss=1.3465, valid acc=0.5967\n",
      "########: train loss=1.2534, train acc=0.6232\n",
      "Epoch 14: valid loss=1.3197, valid acc=0.6007\n",
      "########: train loss=1.2247, train acc=0.6310\n",
      "Epoch 15: valid loss=1.2971, valid acc=0.6067\n",
      "########: train loss=1.1996, train acc=0.6385\n",
      "Epoch 16: valid loss=1.2777, valid acc=0.6073\n",
      "########: train loss=1.1779, train acc=0.6433\n",
      "Epoch 17: valid loss=1.2604, valid acc=0.6067\n",
      "########: train loss=1.1580, train acc=0.6497\n",
      "Epoch 18: valid loss=1.2452, valid acc=0.6113\n",
      "########: train loss=1.1407, train acc=0.6545\n",
      "Epoch 19: valid loss=1.2311, valid acc=0.6140\n",
      "########: train loss=1.1256, train acc=0.6578\n",
      "Epoch 20: valid loss=1.2180, valid acc=0.6193\n",
      "########: train loss=1.1115, train acc=0.6620\n",
      "Epoch 21: valid loss=1.2067, valid acc=0.6180\n",
      "########: train loss=1.0982, train acc=0.6670\n",
      "Epoch 22: valid loss=1.1950, valid acc=0.6193\n",
      "########: train loss=1.0848, train acc=0.6711\n",
      "Epoch 23: valid loss=1.1841, valid acc=0.6227\n",
      "########: train loss=1.0722, train acc=0.6743\n",
      "Epoch 24: valid loss=1.1732, valid acc=0.6273\n",
      "########: train loss=1.0593, train acc=0.6790\n",
      "Epoch 25: valid loss=1.1619, valid acc=0.6293\n",
      "########: train loss=1.0463, train acc=0.6827\n",
      "Epoch 26: valid loss=1.1517, valid acc=0.6340\n",
      "########: train loss=1.0338, train acc=0.6862\n",
      "Epoch 27: valid loss=1.1414, valid acc=0.6400\n",
      "########: train loss=1.0217, train acc=0.6903\n",
      "Epoch 28: valid loss=1.1326, valid acc=0.6440\n",
      "########: train loss=1.0111, train acc=0.6922\n",
      "Epoch 29: valid loss=1.1236, valid acc=0.6447\n",
      "########: train loss=1.0001, train acc=0.6952\n",
      "Epoch 30: valid loss=1.1151, valid acc=0.6493\n",
      "########: train loss=0.9897, train acc=0.6973\n",
      "Epoch 31: valid loss=1.1070, valid acc=0.6513\n",
      "########: train loss=0.9802, train acc=0.6999\n",
      "Epoch 32: valid loss=1.0985, valid acc=0.6520\n",
      "########: train loss=0.9702, train acc=0.7019\n",
      "Epoch 33: valid loss=1.0902, valid acc=0.6533\n",
      "########: train loss=0.9611, train acc=0.7057\n",
      "Epoch 34: valid loss=1.0817, valid acc=0.6540\n",
      "########: train loss=0.9522, train acc=0.7063\n",
      "Epoch 35: valid loss=1.0743, valid acc=0.6580\n",
      "########: train loss=0.9442, train acc=0.7092\n",
      "Epoch 36: valid loss=1.0664, valid acc=0.6627\n",
      "########: train loss=0.9360, train acc=0.7119\n",
      "Epoch 37: valid loss=1.0593, valid acc=0.6653\n",
      "########: train loss=0.9284, train acc=0.7139\n",
      "Epoch 38: valid loss=1.0526, valid acc=0.6740\n",
      "########: train loss=0.9213, train acc=0.7157\n",
      "Epoch 39: valid loss=1.0455, valid acc=0.6787\n",
      "########: train loss=0.9142, train acc=0.7181\n",
      "Epoch 40: valid loss=1.0395, valid acc=0.6813\n",
      "########: train loss=0.9079, train acc=0.7202\n",
      "Epoch 41: valid loss=1.0328, valid acc=0.6833\n",
      "########: train loss=0.9015, train acc=0.7221\n",
      "Epoch 42: valid loss=1.0263, valid acc=0.6860\n",
      "########: train loss=0.8955, train acc=0.7240\n",
      "Epoch 43: valid loss=1.0210, valid acc=0.6860\n",
      "########: train loss=0.8899, train acc=0.7253\n",
      "Epoch 44: valid loss=1.0152, valid acc=0.6860\n",
      "########: train loss=0.8845, train acc=0.7277\n",
      "Epoch 45: valid loss=1.0104, valid acc=0.6880\n",
      "########: train loss=0.8793, train acc=0.7300\n",
      "Epoch 46: valid loss=1.0060, valid acc=0.6867\n",
      "########: train loss=0.8744, train acc=0.7324\n",
      "Epoch 47: valid loss=1.0013, valid acc=0.6867\n",
      "########: train loss=0.8695, train acc=0.7333\n",
      "Epoch 48: valid loss=0.9975, valid acc=0.6867\n",
      "########: train loss=0.8650, train acc=0.7346\n",
      "Epoch 49: valid loss=0.9940, valid acc=0.6867\n",
      "########: train loss=0.8607, train acc=0.7358\n",
      "Epoch 50: valid loss=0.9903, valid acc=0.6880\n",
      "########: train loss=0.8563, train acc=0.7368\n",
      "Epoch 51: valid loss=0.9863, valid acc=0.6920\n",
      "########: train loss=0.8518, train acc=0.7385\n",
      "Epoch 52: valid loss=0.9823, valid acc=0.6940\n",
      "########: train loss=0.8474, train acc=0.7385\n",
      "Epoch 53: valid loss=0.9787, valid acc=0.6940\n",
      "########: train loss=0.8431, train acc=0.7409\n",
      "Epoch 54: valid loss=0.9748, valid acc=0.6953\n",
      "########: train loss=0.8389, train acc=0.7419\n",
      "Epoch 55: valid loss=0.9708, valid acc=0.6967\n",
      "########: train loss=0.8346, train acc=0.7438\n",
      "Epoch 56: valid loss=0.9671, valid acc=0.6980\n",
      "########: train loss=0.8304, train acc=0.7450\n",
      "Epoch 57: valid loss=0.9642, valid acc=0.7013\n",
      "########: train loss=0.8263, train acc=0.7466\n",
      "Epoch 58: valid loss=0.9609, valid acc=0.7007\n",
      "########: train loss=0.8227, train acc=0.7484\n",
      "Epoch 59: valid loss=0.9575, valid acc=0.7033\n",
      "########: train loss=0.8187, train acc=0.7492\n",
      "Epoch 60: valid loss=0.9537, valid acc=0.7047\n",
      "########: train loss=0.8148, train acc=0.7505\n",
      "Epoch 61: valid loss=0.9488, valid acc=0.7067\n",
      "########: train loss=0.8107, train acc=0.7518\n",
      "Epoch 62: valid loss=0.9452, valid acc=0.7087\n",
      "########: train loss=0.8071, train acc=0.7514\n",
      "Epoch 63: valid loss=0.9428, valid acc=0.7073\n",
      "########: train loss=0.8033, train acc=0.7521\n",
      "Epoch 64: valid loss=0.9376, valid acc=0.7100\n",
      "########: train loss=0.7989, train acc=0.7533\n",
      "Epoch 65: valid loss=0.9340, valid acc=0.7107\n",
      "########: train loss=0.7946, train acc=0.7545\n",
      "Epoch 66: valid loss=0.9290, valid acc=0.7120\n",
      "########: train loss=0.7903, train acc=0.7558\n",
      "Epoch 67: valid loss=0.9254, valid acc=0.7120\n",
      "########: train loss=0.7859, train acc=0.7573\n",
      "Epoch 68: valid loss=0.9218, valid acc=0.7133\n",
      "########: train loss=0.7820, train acc=0.7603\n",
      "Epoch 69: valid loss=0.9179, valid acc=0.7140\n",
      "########: train loss=0.7779, train acc=0.7620\n",
      "Epoch 70: valid loss=0.9130, valid acc=0.7187\n",
      "########: train loss=0.7734, train acc=0.7648\n",
      "Epoch 71: valid loss=0.9092, valid acc=0.7200\n",
      "########: train loss=0.7695, train acc=0.7673\n",
      "Epoch 72: valid loss=0.9042, valid acc=0.7233\n",
      "########: train loss=0.7655, train acc=0.7682\n",
      "Epoch 73: valid loss=0.9004, valid acc=0.7233\n",
      "########: train loss=0.7616, train acc=0.7697\n",
      "Epoch 74: valid loss=0.8968, valid acc=0.7267\n",
      "########: train loss=0.7583, train acc=0.7709\n",
      "Epoch 75: valid loss=0.8925, valid acc=0.7293\n",
      "########: train loss=0.7545, train acc=0.7717\n",
      "Epoch 76: valid loss=0.8888, valid acc=0.7300\n",
      "########: train loss=0.7511, train acc=0.7730\n",
      "Epoch 77: valid loss=0.8863, valid acc=0.7333\n",
      "########: train loss=0.7479, train acc=0.7742\n",
      "Epoch 78: valid loss=0.8822, valid acc=0.7347\n",
      "########: train loss=0.7442, train acc=0.7754\n",
      "Epoch 79: valid loss=0.8789, valid acc=0.7373\n",
      "########: train loss=0.7410, train acc=0.7757\n",
      "Epoch 80: valid loss=0.8739, valid acc=0.7373\n",
      "########: train loss=0.7372, train acc=0.7758\n",
      "Epoch 81: valid loss=0.8694, valid acc=0.7380\n",
      "########: train loss=0.7333, train acc=0.7787\n",
      "Epoch 82: valid loss=0.8662, valid acc=0.7407\n",
      "########: train loss=0.7300, train acc=0.7798\n",
      "Epoch 83: valid loss=0.8631, valid acc=0.7427\n",
      "########: train loss=0.7262, train acc=0.7806\n",
      "Epoch 84: valid loss=0.8598, valid acc=0.7427\n",
      "########: train loss=0.7230, train acc=0.7815\n",
      "Epoch 85: valid loss=0.8559, valid acc=0.7447\n",
      "########: train loss=0.7194, train acc=0.7829\n",
      "Epoch 86: valid loss=0.8534, valid acc=0.7433\n",
      "########: train loss=0.7162, train acc=0.7838\n",
      "Epoch 87: valid loss=0.8503, valid acc=0.7427\n",
      "########: train loss=0.7132, train acc=0.7851\n",
      "Epoch 88: valid loss=0.8472, valid acc=0.7440\n",
      "########: train loss=0.7102, train acc=0.7862\n",
      "Epoch 89: valid loss=0.8435, valid acc=0.7440\n",
      "########: train loss=0.7070, train acc=0.7871\n",
      "Epoch 90: valid loss=0.8396, valid acc=0.7480\n",
      "########: train loss=0.7037, train acc=0.7878\n",
      "Epoch 91: valid loss=0.8362, valid acc=0.7480\n",
      "########: train loss=0.7010, train acc=0.7878\n",
      "Epoch 92: valid loss=0.8324, valid acc=0.7493\n",
      "########: train loss=0.6979, train acc=0.7895\n",
      "Epoch 93: valid loss=0.8294, valid acc=0.7513\n",
      "########: train loss=0.6951, train acc=0.7903\n",
      "Epoch 94: valid loss=0.8258, valid acc=0.7500\n",
      "########: train loss=0.6927, train acc=0.7912\n",
      "Epoch 95: valid loss=0.8229, valid acc=0.7473\n",
      "########: train loss=0.6896, train acc=0.7918\n",
      "Epoch 96: valid loss=0.8197, valid acc=0.7533\n",
      "########: train loss=0.6872, train acc=0.7930\n",
      "Epoch 97: valid loss=0.8169, valid acc=0.7527\n",
      "########: train loss=0.6845, train acc=0.7953\n",
      "Epoch 98: valid loss=0.8141, valid acc=0.7553\n",
      "########: train loss=0.6819, train acc=0.7958\n",
      "Epoch 99: valid loss=0.8109, valid acc=0.7540\n",
      "########: train loss=0.6793, train acc=0.7962\n",
      "Epoch 100: valid loss=0.8083, valid acc=0.7540\n",
      "########: train loss=0.6767, train acc=0.7974\n",
      "---------------------------------------------\n",
      "\n",
      "Test accuracy: 0.7813\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "batch_size= 32\n",
    "history= []  # used to store train, valid accuracies and losses for showing later\n",
    "num_epoch = 100 # training stops at 100 epochs\n",
    "iter_per_epoch= math.ceil(float(train_size)/batch_size)  #number of iterations per epoch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init= tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    # ME: what is difference between epoch and iteration? \n",
    "    # ANS: if we have 1000 data, use 100 per batch,\n",
    "    # this means 10 iterations (1000/100 = 10), \n",
    "    # when the 1000 data is scanned, it is 1 epoch\n",
    "    for epoch in range(num_epoch):\n",
    "        for idx_start in range(0, X_train.shape[0], batch_size):\n",
    "            idx_end = min(X_train.shape[0], idx_start + batch_size)\n",
    "            X_batch, y_batch = X_train[idx_start:idx_end], y_train[idx_start:idx_end]\n",
    "            sess.run([train_op], feed_dict={X:X_batch, y:y_batch})\n",
    "        #compute accuracies and losses at the end of each epoch\n",
    "        # ME: most of the time, we only want to get the results after the epoch\n",
    "        train_summary, train_loss, train_accuracy= sess.run([merged,loss, accuracy], feed_dict={X:X_train, y:y_train})\n",
    "        train_writer.add_summary(train_summary, epoch +1)\n",
    "        train_writer.flush()\n",
    "        \n",
    "        # ME: what is VALIDATION LOSS? (same logic applies to TRAINING LOSS)\n",
    "        # this is the loss when we run through the validation dataset (unseen data for the model)\n",
    "        # after model is fully trained, then we use test set, because we dont want to overfit onto the test set\n",
    "        # QUES: when does overfit occur? train acc very high, valid acc very low.\n",
    "        valid_summary,val_loss, val_accuracy= sess.run([merged,loss, accuracy], feed_dict={X:X_valid, y:y_valid})\n",
    "        valid_writer.add_summary(valid_summary, epoch +1)\n",
    "        valid_writer.flush()\n",
    "        print(\"Epoch {}: valid loss={:.4f}, valid acc={:.4f}\".format(epoch+1, val_loss, val_accuracy))\n",
    "        print(\"########: train loss={:.4f}, train acc={:.4f}\".format(train_loss, train_accuracy))\n",
    "        hist_item={\"train_loss\": train_loss, \"train_acc\": train_accuracy, \n",
    "                   \"val_loss\":val_loss, \"val_acc\": val_accuracy}\n",
    "        history.append(hist_item)\n",
    "    print(\"---------------------------------------------\\n\")\n",
    "    test_accuracy= sess.run(accuracy, feed_dict={X:X_test, y:y_test})\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> Additional Exercises </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write your own code to save a trained model to the hard disk and restore this model, then use the restored model to output the prediction result on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write code to add the plots of `test accuracy` and `loss` to the above line charts with your color of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Insert new code to the above code to enable outputting to TensorBoard the values of `training loss`, `training accuracy`, `valid loss`, and `valid accuracy` at the end of epochs. You can refer to the code [here](https://www.tensorflow.org/guide/summaries_and_tensorboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write code to do regression on the dataset `cadata` which can be downloaded [here](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html). Note that for a regression problem, you need to use the `L2` loss instead of the `cross-entropy` loss as in a classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6c8f846148a3e4d140e6ddf63c190cff559dcf260a4a21539f0978f2b58638c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
