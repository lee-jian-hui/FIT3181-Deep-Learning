{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT5215: Deep Learning (2022)</span>\n",
    "***\n",
    "*CE/Lecturer:*  **Dr Trung Le** | trunglm@monash.edu <br/> <br/>\n",
    "*Tutor:*  **Mr Tuan Nguyen**  \\[tuan.ng@monash.edu \\] |**Mr Anh Bui** \\[tuananh.bui@monash.edu\\] | **Mr Xiaohao Yang** \\[xiaohao.yang@monash.edu \\] | **Mr Md Mohaimenuzzaman** \\[md.mohaimen@monash.edu \\] |**Mr Thanh Nguyen** \\[Thanh.Nguyen4@monash.edu \\] |\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Tutorial 3a: Feed-forward Neural Nets with TensorFlow 1.x</span> \n",
    "**The purpose of this tutorial is to demonstrate how to work with an open source software library for developing deep neural networks apllications, called TensorFlow. In this tutorial, we will focus on**:  \n",
    "- ***Inspect the common pipeline of deep learning*.**\n",
    "- ***How to implement a feedforward neural net for a multi-class classfication problem using TF 1.x in Tutorial 3a (this tutorial)*.**\n",
    "- ***How to implement a feedforward neural net for a multi-class classfication problem using TF 2.x in Tutorial 3b*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> II.1 Feedforward Neural Network </span> <span style=\"color:red\">***** (highly important)</span>\n",
    "#### <span style=\"color:#0b486b\"> Tutorial objective </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will consider a fairly realistic deep NNs with *three* layers plus the *output* layer. Its architecture will be specified as: $16 \\rightarrow 10 (ReLU) \\rightarrow 20 (ReLU) \\rightarrow 15 (ReLu) \\rightarrow 26$. This means:\n",
    "- Input size is 16\n",
    "- First layer has 10 hidden units with ReLU activation functions\n",
    "- Second layer has 20 hidden units with 20 ReLU activiation functions\n",
    "- Third layer has 15 hidden units with 15 ReLU activiation functions\n",
    "- And output layer is logit layer with 26 hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network, for example, can take the `letter` dataset input with $16$ features and with $26$ classes (A-Z). **Our objective in this tutorial is to implement this specific network in `TensorFlow 1.x`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Specifying the Neural Network Architecture </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this network as in the figure below. Please note that for readability, the number of hidden units in the figure might not correspond exactly to the actual size of the hidden units used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DNN_Pipeline.PNG\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/classpic1.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in this picture above, input layer has 32 samples with 16 features\n",
    "- the 2nd layer will have 10 neurons only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the above figure shows the pipeline of the entire process for feeding a mini-batch of batch size $32$ into the network. Using ***mini-batch*** is a common way to train deep NNs in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us denote the mini-batch by $X_b= \\{(x_1, y_1),\\dots, (x_{32}, y_{32})\\}$. The mini-batch can be stored using a $2D$ tensor with the shape $(32, 16)$. **Assume that in this network, we use the activation function $ReLu$ where $ReLu(t)= \\max\\{0, t\\}$**. The computation in the forward propagation step is as follows:\n",
    "- Input $X_b$ with mini-batch size of 32\n",
    "- $h_1= ReLu(X_b \\times W^1 + b^1)\\in \\mathbb{R}^{32 \\times 10}$. \n",
    "- $h_2= ReLu(h_1 \\times W^2 + b^2\\in \\mathbb{R}^{32 \\times 20}$. \n",
    "- $h_3= ReLu(h_2 \\times W^3 + b^3\\in \\mathbb{R}^{32 \\times 15}$. \n",
    "- $logits= h_3 \\times W^4 + b^4 \\in \\mathbb{R}^{32 \\times 26}$\n",
    "- $p = softmax(logits) \\in \\mathbb{R}^{32 \\times 26}$ <br>\n",
    "\n",
    "where we note that the activation function is perfomed element-wise and the softmax function is used to transform a vector of scalars to a discrete distribution as: \n",
    "\n",
    "$$softmax(z)=\\big[\\frac{\\exp(z_i)}{\\sum_{j=1}^{26}{\\exp(z_j)}}\\big]_{i=1}^{26}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "The $k$-th row $p_k$ of the matrix $p$ can represent the probability distribution to classify the data point $x_k$ to the classes $1,2,\\dots,26$. In particular, we have:\n",
    "\n",
    "$$p_{km}= p(y_k=m \\mid x_k)  \\text{ for }  m=1,2,\\dots,26$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\"> Exercise 1</span>** : Explain why the dimension for $h_1$ is $32\\times 10$? Similarly, please work out the dimension for $h2, h3, logits$ and $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### definition of h_i:\n",
    "- the output after passing through the i-th layer (linear transformation of product with weight and addition with bias)\n",
    "- which is passed through the ReLu activation function\n",
    "\n",
    "ANS: \n",
    "- because input layer has 32 samples and 16 features, \n",
    "- to accomodate this matrix, and perform matrix mult,\n",
    "- input layer has 32 samples with 16 features each\n",
    "- however, the 2nd layer only has 10 neurons\n",
    "- (32 x 16) (16 x 10) = (32x10)\n",
    "- it looks like the 2nd layer is interested in reducing number of features we want to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Specifying the Loss Function </span>\n",
    "Essiential to training a deep NN is the concept of the **loss function**. This function will tell us how good the network is predicting, and hence we can use this loss to find the network weights in such a way that the loss can be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification task, a common approach is to use the **cross-entropy** loss function. Given a data-label instance $(x_k,y_k)$ where feature $x_k\\in \\mathbb{R}^{16}$ and the label $y_k \\in \\{1,2,...,26\\}$ is a numeric label (for example if $x_k$ is in the class 2, then $y_k =2 $ and its one-hot vector $1_{y_k}=[0,1,0,...,0]$). The cross-entroty between the classification distribution $p_k$ returned from the NN and true label distribution $y_k$ is defined as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$cross\\_entropy(1_{y_k}, p_k)=-\\sum_{j=1}^{26}y_{kj} \\log{p_{kj}}= -\\log p_{k,y_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss basically enforces the model to predict the label as close as the true label by minimizing $cross\\_entropy(1_{y_k}, p_k)$.\n",
    "\n",
    "The above loss function was applied for each instance. For the entire current mini-batch, our loss function becomes: \n",
    "$$\\min \\sum_{k=1}^{32}cross\\_entropy(1_{y_k}, p_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\"> Exercise 2: </span>** : **<span style=\"color:#0b486b\">In the corss-entropy equation above, $y_k$ is the class for $x_k$, explain why the end result is $-\\log p_{k,y_k}$.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS: The reason is that only $y_{km_k}=1$ and others $y_{km_j}=0, \\forall j \\neq m_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\"> Exercise 3: </span>** : **<span style=\"color:#0b486b\">Let $p=[0.1, 0.3, 0.6]$ and $q=[0.0, 0.5, 0.5]$ be two discrete distributions, what is the $cross\\_entropy(q,p)$ ?</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8573992140459634\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "p = np.array([0.1, 0.3, 0.6])\n",
    "q = np.array([0.0, 0.5, 0.5])\n",
    "ce = -np.sum(q * np.log(p))   # sum of the negative logs\n",
    "print(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> II.2 Implementation with TensorFlow 1.x</span> <span style=\"color:red\">**** (important)</span>\n",
    "We now shall implement the aforementioned network with the architecture of $16 \\rightarrow 10 (ReLU) \\rightarrow 20 (ReLU) \\rightarrow 15 (ReLu) \\rightarrow 26$ in Tensorflow using the dataset `letter`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This letter dataset can be found at [the LIBSVM website](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#letter). Here is the dataset information:\n",
    "-  *The objective is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical pipeline process of implementing a deep learning model is as follows:\n",
    "\n",
    "1. **Data processing**: \n",
    "   - Load the dataset and split into train, valid, and test sets.  \n",
    "     \n",
    "2. **Construction phase**: \n",
    "   - Define the NN model and construct the corresponding computational graph.\n",
    "   - Define the loss function and the relevant measures of performance of interest (accuracy, F1, and AUC).\n",
    "    \n",
    "3. **Execution and evaluation phase**: \n",
    "   - Train the model using mini-batches from the train set by minimizing the loss function with an optimizer.\n",
    "   - Predict on the test set and access its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">1. Data Processing </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `sklearn` to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X data shape: (15000, 16)\n",
      "y data shape: (15000, 1)\n",
      "# classes: 26\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      " 19. 20. 21. 22. 23. 24. 25. 26.]\n"
     ]
    }
   ],
   "source": [
    "data_file_name= \"letter_scale.libsvm\"\n",
    "data_file = os.path.abspath(\"./Data/\" + data_file_name)\n",
    "X_data, y_data = load_svmlight_file(data_file)\n",
    "X_data= X_data.toarray()\n",
    "y_data= y_data.reshape(y_data.shape[0],-1)\n",
    "print(\"X data shape: {}\".format(X_data.shape))\n",
    "print(\"y data shape: {}\".format(y_data.shape))\n",
    "print(\"# classes: {}\".format(len(np.unique(y_data))))\n",
    "print(np.unique(y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `sklearn` to split the dataset into the train, validation, and test sets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\PC\\anaconda3\\envs\\tf2_cpu\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "def train_valid_test_split(data, target, train_size, test_size):\n",
    "    valid_size = 1 - (train_size + test_size)\n",
    "    X1, X_test, y1, y_test = train_test_split(data, target, test_size = test_size, random_state= 33)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X1, y1, test_size = float(valid_size)/(valid_size+ train_size))\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would like to encode the label in the form of numeric vector. For example, we want to turn $y\\_data=[\"cat\", \"dog\", \"cat\", \"lion\", \"dog\"]$ to $y\\_data=[0,1,0,2,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, in the following segment of code, we use the object `le` as an instance of the class `preprocessing.LabelEncoder()` which supports us to transform catefgorial labels in `y_data` to numerical vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 15 18  7  7  5 13 17 12  3 21  0 10  3 18  7  4 25 14 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\tf2_cpu\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\PC\\anaconda3\\envs\\tf2_cpu\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_data)\n",
    "y_data= le.transform(y_data)\n",
    "print(y_data[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the function defined above to prepare our data for training, validating and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 16) (1500, 16) (1500, 16)\n",
      "(12000,) (1500,) (1500,)\n",
      "lables: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test = train_valid_test_split(X_data, y_data, train_size=0.8, test_size=0.1)\n",
    "y_train= y_train.reshape(-1)\n",
    "y_test= y_test.reshape(-1)\n",
    "y_valid= y_valid.reshape(-1)\n",
    "print(X_train.shape, X_valid.shape, X_test.shape)\n",
    "print(y_train.shape, y_valid.shape, y_test.shape)\n",
    "print(\"lables: {}\".format(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We catch some information of the training set which will be reused later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size= int(X_train.shape[0])\n",
    "n_features= int(X_train.shape[1])\n",
    "n_classes= len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, in real-world implementation of deep learning models, we use Stochastic Gradient Descent (SGD). Input to this algorithm is a sequence of **mini-batch** of data drawn from the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">2. Construction Phase </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build up a feedforward neural network with the architecture: $16 \\rightarrow 10 (ReLU) \\rightarrow 20 (ReLU) \\rightarrow 15 (ReLu) \\rightarrow 26$ in TensorFlow. \n",
    "\n",
    ":- these are the number of neurons at each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in= n_features    # dimension of input\n",
    "n1= 10              # number of hidden units at the first layer\n",
    "n2= 20              # number of hidden units at the second layer\n",
    "n3= 15              # number of hidden units at the third layer\n",
    "n_out= n_classes    # number of classification classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `dense_layer` represents a fully connected layer in a deep learning network. This takes $W,b$ and input as inputs and returns $\\sigma(W \\times input + b)$ where the activation function $sigma$ is specified by the parameter `act`.\n",
    "- In `TensorFlow`, we can refer to the `activation functions` as `tf.nn.relu`, `tf.nn.sigmoid`, `tf.nn.tanh`, and etc.\n",
    "- You can also define your own activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 defining dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(inputs, output_size, act=None, name=\"hidden-layer\"):\n",
    "    with tf.name_scope(name):\n",
    "        input_size= int(inputs.get_shape()[1])\n",
    "        W_init = tf.random.normal([input_size, output_size], mean=0, stddev= 0.1, dtype= tf.float32)\n",
    "        b_init= tf.random.normal([output_size], mean=0, stddev= 0.1, dtype= tf.float32)\n",
    "        W= tf.Variable(W_init, name= \"W\")\n",
    "        b= tf.Variable(b_init, name=\"b\")\n",
    "        Wxb= tf.matmul(inputs, W) + b\n",
    "        if act is None:\n",
    "            return Wxb\n",
    "        else:\n",
    "            return act(Wxb)  # activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 construct neural network graph\n",
    "\n",
    "**We now construct the computational graph**. But before that we need to reset the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.name_scope(\"network\"):\n",
    "    X= tf.placeholder(shape=[None, n_in], dtype= tf.float32)\n",
    "    y= tf.placeholder(shape=[None], dtype= tf.int32)\n",
    "    h1= dense_layer(X, n1, act= tf.nn.relu, name= \"layer1\")\n",
    "    h2= dense_layer(h1, n2, act= tf.nn.relu, name= \"layer2\")\n",
    "    h3= dense_layer(h2, n3, act= tf.nn.relu, name= \"layer3\")\n",
    "    logits= dense_layer(h3, n_out, name=\"logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the cross-entropy loss. Note that in TensorFlow you can use two of following functions for evaluating the cross-entropy loss:\n",
    "- `tf.nn.sparse_softmax_cross_entropy_with_logits`: if the labels `y_train` is in the categorial format (e.g., `y_train=[0,1,0,1,1,2]`).\n",
    "- `tf.nn.softmax_cross_entropy_with_logits`: if the labels `y_train` is in the one-hot format (e.g., `y_train=[[1,0,0], [0,1,0], [1,0,0], [0,0,1]]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 train with optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to specify an optimizer to minimize the loss. Here, we are using the Adam optimizer for this optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \n",
    "                                                              logits=logits, \n",
    "                                                              name='xentropy')\n",
    "    loss= tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    tf.summary.scalar(\"loss\", loss)    #summarize the loss\n",
    "    optimizer= tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_op= optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4 defining performance evaluation variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we add the line ***tf.summary.scalar(\"loss\", loss)*** to add to the summary the loss.\n",
    "\n",
    "We also wish to estimate the accuracy of our model. For this you can use the [*`in_top_k()`* function](https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k) with *k=1*. This returns a 1D tensor full of boolean values, so we need to cast these booleans to floats and then compute the average. This will give us the network’s overall accuracy. We insert the line ***tf.summary.scalar(\"accuracy\", accuracy)*** to add to the summary the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('evaluation'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)     # gets the top 1 result\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)  #summarize the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define two FileWriters to write the summary to two log folders. By this way, we can plot the train, valid losses (or accuracies) on the same graph. Note that you can use this trick when you want to display some plots on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.exists(\"./logs/train\")):\n",
    "    os.makedirs(\"./logs/train\")\n",
    "\n",
    "if(not os.path.exists(\"./logs/val\")):\n",
    "    os.makedirs(\"./logs/val\")\n",
    "\n",
    "merged= tf.summary.merge_all()\n",
    "train_writer= tf.summary.FileWriter(\"./logs/train\")\n",
    "valid_writer= tf.summary.FileWriter(\"./logs/val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">3. Execution and Evaluation Phase </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `execution phase`, we need to create a `TensorFlow session`, then initialize `all variables` in the graph, execute `train_op`, and query the values of necessary nodes (e.g., `loss` and `accuracy`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize all variables\n",
    "  - `init= tf.global_variables_initializer()` and `sess.run(init)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Execute `train_op` when feeding mini-batches to the network\n",
    "  - `sess.run([train_op], feed_dict={X:x_batch, y:y_batch})`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query the values of necessary nodes\n",
    "  - `val_loss, val_accuracy= sess.run([loss, accuracy], feed_dict={X:X_valid, y:y_valid})`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as a rule of machine learning, during training phase, we **cannot** touch the `test set` and only use this set when we need to output the predictive performance of a trained model.\n",
    "-  Output the predictive performance on the test set\n",
    "   - `test_accuracy= sess.run(accuracy, feed_dict={X:X_test, y:y_test})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: valid loss=2.5008, valid acc=0.2307\n",
      "########: train loss=2.4708, train acc=0.2463\n",
      "Epoch 2: valid loss=2.1129, valid acc=0.3313\n",
      "########: train loss=2.0725, train acc=0.3540\n",
      "Epoch 3: valid loss=1.9686, valid acc=0.3887\n",
      "########: train loss=1.9250, train acc=0.4052\n",
      "Epoch 4: valid loss=1.8458, valid acc=0.4253\n",
      "########: train loss=1.8030, train acc=0.4435\n",
      "Epoch 5: valid loss=1.7768, valid acc=0.4447\n",
      "########: train loss=1.7323, train acc=0.4581\n",
      "Epoch 6: valid loss=1.7327, valid acc=0.4540\n",
      "########: train loss=1.6846, train acc=0.4668\n",
      "Epoch 7: valid loss=1.6934, valid acc=0.4687\n",
      "########: train loss=1.6424, train acc=0.4778\n",
      "Epoch 8: valid loss=1.6591, valid acc=0.4847\n",
      "########: train loss=1.6038, train acc=0.4953\n",
      "Epoch 9: valid loss=1.6287, valid acc=0.4993\n",
      "########: train loss=1.5682, train acc=0.5117\n",
      "Epoch 10: valid loss=1.5988, valid acc=0.5100\n",
      "########: train loss=1.5336, train acc=0.5228\n",
      "Epoch 11: valid loss=1.5672, valid acc=0.5287\n",
      "########: train loss=1.4986, train acc=0.5350\n",
      "Epoch 12: valid loss=1.5362, valid acc=0.5327\n",
      "########: train loss=1.4658, train acc=0.5473\n",
      "Epoch 13: valid loss=1.5056, valid acc=0.5420\n",
      "########: train loss=1.4349, train acc=0.5589\n",
      "Epoch 14: valid loss=1.4752, valid acc=0.5547\n",
      "########: train loss=1.4054, train acc=0.5677\n",
      "Epoch 15: valid loss=1.4454, valid acc=0.5667\n",
      "########: train loss=1.3769, train acc=0.5807\n",
      "Epoch 16: valid loss=1.4162, valid acc=0.5733\n",
      "########: train loss=1.3493, train acc=0.5910\n",
      "Epoch 17: valid loss=1.3878, valid acc=0.5807\n",
      "########: train loss=1.3224, train acc=0.5987\n",
      "Epoch 18: valid loss=1.3609, valid acc=0.5880\n",
      "########: train loss=1.2970, train acc=0.6090\n",
      "Epoch 19: valid loss=1.3348, valid acc=0.5993\n",
      "########: train loss=1.2724, train acc=0.6157\n",
      "Epoch 20: valid loss=1.3100, valid acc=0.6060\n",
      "########: train loss=1.2486, train acc=0.6234\n",
      "Epoch 21: valid loss=1.2857, valid acc=0.6160\n",
      "########: train loss=1.2254, train acc=0.6292\n",
      "Epoch 22: valid loss=1.2620, valid acc=0.6207\n",
      "########: train loss=1.1993, train acc=0.6348\n",
      "Epoch 23: valid loss=1.2386, valid acc=0.6300\n",
      "########: train loss=1.1734, train acc=0.6416\n",
      "Epoch 24: valid loss=1.2164, valid acc=0.6380\n",
      "########: train loss=1.1494, train acc=0.6479\n",
      "Epoch 25: valid loss=1.1942, valid acc=0.6467\n",
      "########: train loss=1.1261, train acc=0.6558\n",
      "Epoch 26: valid loss=1.1737, valid acc=0.6567\n",
      "########: train loss=1.1051, train acc=0.6597\n",
      "Epoch 27: valid loss=1.1541, valid acc=0.6607\n",
      "########: train loss=1.0857, train acc=0.6646\n",
      "Epoch 28: valid loss=1.1348, valid acc=0.6660\n",
      "########: train loss=1.0679, train acc=0.6692\n",
      "Epoch 29: valid loss=1.1164, valid acc=0.6740\n",
      "########: train loss=1.0512, train acc=0.6718\n",
      "Epoch 30: valid loss=1.0994, valid acc=0.6813\n",
      "########: train loss=1.0358, train acc=0.6753\n",
      "Epoch 31: valid loss=1.0830, valid acc=0.6913\n",
      "########: train loss=1.0218, train acc=0.6819\n",
      "Epoch 32: valid loss=1.0683, valid acc=0.6973\n",
      "########: train loss=1.0091, train acc=0.6867\n",
      "Epoch 33: valid loss=1.0554, valid acc=0.7027\n",
      "########: train loss=0.9975, train acc=0.6921\n",
      "Epoch 34: valid loss=1.0432, valid acc=0.7093\n",
      "########: train loss=0.9866, train acc=0.6957\n",
      "Epoch 35: valid loss=1.0327, valid acc=0.7080\n",
      "########: train loss=0.9762, train acc=0.6993\n",
      "Epoch 36: valid loss=1.0224, valid acc=0.7127\n",
      "########: train loss=0.9663, train acc=0.7027\n",
      "Epoch 37: valid loss=1.0128, valid acc=0.7133\n",
      "########: train loss=0.9571, train acc=0.7072\n",
      "Epoch 38: valid loss=1.0048, valid acc=0.7120\n",
      "########: train loss=0.9481, train acc=0.7103\n",
      "Epoch 39: valid loss=0.9979, valid acc=0.7167\n",
      "########: train loss=0.9397, train acc=0.7127\n",
      "Epoch 40: valid loss=0.9912, valid acc=0.7153\n",
      "########: train loss=0.9318, train acc=0.7155\n",
      "Epoch 41: valid loss=0.9853, valid acc=0.7153\n",
      "########: train loss=0.9242, train acc=0.7193\n",
      "Epoch 42: valid loss=0.9796, valid acc=0.7167\n",
      "########: train loss=0.9165, train acc=0.7222\n",
      "Epoch 43: valid loss=0.9741, valid acc=0.7207\n",
      "########: train loss=0.9090, train acc=0.7245\n",
      "Epoch 44: valid loss=0.9691, valid acc=0.7233\n",
      "########: train loss=0.9021, train acc=0.7282\n",
      "Epoch 45: valid loss=0.9647, valid acc=0.7253\n",
      "########: train loss=0.8959, train acc=0.7297\n",
      "Epoch 46: valid loss=0.9599, valid acc=0.7260\n",
      "########: train loss=0.8896, train acc=0.7318\n",
      "Epoch 47: valid loss=0.9558, valid acc=0.7273\n",
      "########: train loss=0.8836, train acc=0.7337\n",
      "Epoch 48: valid loss=0.9515, valid acc=0.7320\n",
      "########: train loss=0.8777, train acc=0.7358\n",
      "Epoch 49: valid loss=0.9472, valid acc=0.7320\n",
      "########: train loss=0.8722, train acc=0.7377\n",
      "Epoch 50: valid loss=0.9433, valid acc=0.7287\n",
      "########: train loss=0.8671, train acc=0.7397\n",
      "Epoch 51: valid loss=0.9394, valid acc=0.7293\n",
      "########: train loss=0.8620, train acc=0.7410\n",
      "Epoch 52: valid loss=0.9360, valid acc=0.7347\n",
      "########: train loss=0.8572, train acc=0.7422\n",
      "Epoch 53: valid loss=0.9318, valid acc=0.7347\n",
      "########: train loss=0.8524, train acc=0.7433\n",
      "Epoch 54: valid loss=0.9280, valid acc=0.7347\n",
      "########: train loss=0.8479, train acc=0.7445\n",
      "Epoch 55: valid loss=0.9243, valid acc=0.7340\n",
      "########: train loss=0.8432, train acc=0.7452\n",
      "Epoch 56: valid loss=0.9205, valid acc=0.7347\n",
      "########: train loss=0.8386, train acc=0.7465\n",
      "Epoch 57: valid loss=0.9165, valid acc=0.7380\n",
      "########: train loss=0.8340, train acc=0.7483\n",
      "Epoch 58: valid loss=0.9122, valid acc=0.7387\n",
      "########: train loss=0.8294, train acc=0.7505\n",
      "Epoch 59: valid loss=0.9086, valid acc=0.7393\n",
      "########: train loss=0.8251, train acc=0.7517\n",
      "Epoch 60: valid loss=0.9048, valid acc=0.7420\n",
      "########: train loss=0.8207, train acc=0.7537\n",
      "Epoch 61: valid loss=0.9003, valid acc=0.7413\n",
      "########: train loss=0.8162, train acc=0.7546\n",
      "Epoch 62: valid loss=0.8963, valid acc=0.7427\n",
      "########: train loss=0.8117, train acc=0.7553\n",
      "Epoch 63: valid loss=0.8920, valid acc=0.7420\n",
      "########: train loss=0.8075, train acc=0.7556\n",
      "Epoch 64: valid loss=0.8884, valid acc=0.7420\n",
      "########: train loss=0.8034, train acc=0.7563\n",
      "Epoch 65: valid loss=0.8843, valid acc=0.7453\n",
      "########: train loss=0.7993, train acc=0.7570\n",
      "Epoch 66: valid loss=0.8804, valid acc=0.7473\n",
      "########: train loss=0.7945, train acc=0.7581\n",
      "Epoch 67: valid loss=0.8760, valid acc=0.7500\n",
      "########: train loss=0.7901, train acc=0.7598\n",
      "Epoch 68: valid loss=0.8721, valid acc=0.7480\n",
      "########: train loss=0.7861, train acc=0.7611\n",
      "Epoch 69: valid loss=0.8680, valid acc=0.7487\n",
      "########: train loss=0.7818, train acc=0.7626\n",
      "Epoch 70: valid loss=0.8636, valid acc=0.7493\n",
      "########: train loss=0.7771, train acc=0.7645\n",
      "Epoch 71: valid loss=0.8595, valid acc=0.7487\n",
      "########: train loss=0.7728, train acc=0.7659\n",
      "Epoch 72: valid loss=0.8554, valid acc=0.7493\n",
      "########: train loss=0.7684, train acc=0.7678\n",
      "Epoch 73: valid loss=0.8518, valid acc=0.7487\n",
      "########: train loss=0.7644, train acc=0.7687\n",
      "Epoch 74: valid loss=0.8478, valid acc=0.7487\n",
      "########: train loss=0.7601, train acc=0.7696\n",
      "Epoch 75: valid loss=0.8438, valid acc=0.7540\n",
      "########: train loss=0.7557, train acc=0.7711\n",
      "Epoch 76: valid loss=0.8399, valid acc=0.7547\n",
      "########: train loss=0.7518, train acc=0.7714\n",
      "Epoch 77: valid loss=0.8359, valid acc=0.7540\n",
      "########: train loss=0.7477, train acc=0.7731\n",
      "Epoch 78: valid loss=0.8320, valid acc=0.7560\n",
      "########: train loss=0.7438, train acc=0.7747\n",
      "Epoch 79: valid loss=0.8280, valid acc=0.7560\n",
      "########: train loss=0.7401, train acc=0.7757\n",
      "Epoch 80: valid loss=0.8238, valid acc=0.7600\n",
      "########: train loss=0.7360, train acc=0.7769\n",
      "Epoch 81: valid loss=0.8202, valid acc=0.7600\n",
      "########: train loss=0.7324, train acc=0.7778\n",
      "Epoch 82: valid loss=0.8168, valid acc=0.7593\n",
      "########: train loss=0.7287, train acc=0.7784\n",
      "Epoch 83: valid loss=0.8138, valid acc=0.7580\n",
      "########: train loss=0.7249, train acc=0.7803\n",
      "Epoch 84: valid loss=0.8101, valid acc=0.7593\n",
      "########: train loss=0.7212, train acc=0.7814\n",
      "Epoch 85: valid loss=0.8071, valid acc=0.7607\n",
      "########: train loss=0.7178, train acc=0.7832\n",
      "Epoch 86: valid loss=0.8039, valid acc=0.7607\n",
      "########: train loss=0.7144, train acc=0.7837\n",
      "Epoch 87: valid loss=0.8008, valid acc=0.7627\n",
      "########: train loss=0.7113, train acc=0.7849\n",
      "Epoch 88: valid loss=0.7984, valid acc=0.7647\n",
      "########: train loss=0.7087, train acc=0.7854\n",
      "Epoch 89: valid loss=0.7956, valid acc=0.7640\n",
      "########: train loss=0.7055, train acc=0.7870\n",
      "Epoch 90: valid loss=0.7929, valid acc=0.7660\n",
      "########: train loss=0.7028, train acc=0.7878\n",
      "Epoch 91: valid loss=0.7898, valid acc=0.7667\n",
      "########: train loss=0.6998, train acc=0.7885\n",
      "Epoch 92: valid loss=0.7871, valid acc=0.7693\n",
      "########: train loss=0.6972, train acc=0.7882\n",
      "Epoch 93: valid loss=0.7849, valid acc=0.7720\n",
      "########: train loss=0.6946, train acc=0.7883\n",
      "Epoch 94: valid loss=0.7827, valid acc=0.7733\n",
      "########: train loss=0.6924, train acc=0.7893\n",
      "Epoch 95: valid loss=0.7802, valid acc=0.7740\n",
      "########: train loss=0.6899, train acc=0.7899\n",
      "Epoch 96: valid loss=0.7780, valid acc=0.7747\n",
      "########: train loss=0.6873, train acc=0.7902\n",
      "Epoch 97: valid loss=0.7761, valid acc=0.7760\n",
      "########: train loss=0.6851, train acc=0.7918\n",
      "Epoch 98: valid loss=0.7733, valid acc=0.7753\n",
      "########: train loss=0.6826, train acc=0.7933\n",
      "Epoch 99: valid loss=0.7714, valid acc=0.7747\n",
      "########: train loss=0.6801, train acc=0.7938\n",
      "Epoch 100: valid loss=0.7695, valid acc=0.7740\n",
      "########: train loss=0.6781, train acc=0.7943\n",
      "---------------------------------------------\n",
      "\n",
      "Test accuracy: 0.7693\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "batch_size= 32\n",
    "history= []  #used to store train, valid accuracies and losses for showing later\n",
    "num_epoch = 100\n",
    "iter_per_epoch= math.ceil(float(train_size)/batch_size)  #number of iterations per epoch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init= tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epoch):\n",
    "        for idx_start in range(0, X_train.shape[0], batch_size):\n",
    "            idx_end = min(X_train.shape[0], idx_start + batch_size)\n",
    "            X_batch, y_batch = X_train[idx_start:idx_end], y_train[idx_start:idx_end]\n",
    "            sess.run([train_op], feed_dict={X:X_batch, y:y_batch})\n",
    "        #compute accuracies and losses at the end of each epoch\n",
    "        train_summary, train_loss, train_accuracy= sess.run([merged,loss, accuracy], feed_dict={X:X_train, y:y_train})\n",
    "        train_writer.add_summary(train_summary, epoch +1)\n",
    "        train_writer.flush()\n",
    "        \n",
    "        valid_summary,val_loss, val_accuracy= sess.run([merged,loss, accuracy], feed_dict={X:X_valid, y:y_valid})\n",
    "        valid_writer.add_summary(valid_summary, epoch +1)\n",
    "        valid_writer.flush()\n",
    "        print(\"Epoch {}: valid loss={:.4f}, valid acc={:.4f}\".format(epoch+1, val_loss, val_accuracy))\n",
    "        print(\"########: train loss={:.4f}, train acc={:.4f}\".format(train_loss, train_accuracy))\n",
    "        hist_item={\"train_loss\": train_loss, \"train_acc\": train_accuracy, \n",
    "                   \"val_loss\":val_loss, \"val_acc\": val_accuracy}\n",
    "        history.append(hist_item)\n",
    "    print(\"---------------------------------------------\\n\")\n",
    "    test_accuracy= sess.run(accuracy, feed_dict={X:X_test, y:y_test})\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> Additional Exercises </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write your own code to save a trained model to the hard disk and restore this model, then use the restored model to output the prediction result on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write code to add the plots of `test accuracy` and `loss` to the above line charts with your color of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Insert new code to the above code to enable outputting to TensorBoard the values of `training loss`, `training accuracy`, `valid loss`, and `valid accuracy` at the end of epochs. You can refer to the code [here](https://www.tensorflow.org/guide/summaries_and_tensorboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write code to do regression on the dataset `cadata` which can be downloaded [here](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html). Note that for a regression problem, you need to use the `L2` loss instead of the `cross-entropy` loss as in a classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#FFA500\"> Solution for exercises 1, 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now define two Tensorflow FileWriters to write the summary to two log folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.exists(\"logs/tf1/train\")):\n",
    "    os.makedirs(\"logs/tf1/train\")\n",
    "\n",
    "if(not os.path.exists(\"logs/tf1/val\")):\n",
    "    os.makedirs(\"logs/tf1/val\")\n",
    "    \n",
    "if(not os.path.exists(\"logs/tf1/test\")):\n",
    "    os.makedirs(\"logs/tf1/test\")\n",
    "\n",
    "if(not os.path.exists(\"ckpt/tf1\")):\n",
    "    os.makedirs(\"ckpt/tf1\")\n",
    "    \n",
    "merged= tf.summary.merge_all()\n",
    "train_writer= tf.summary.FileWriter(\"logs/tf1/train\")\n",
    "valid_writer= tf.summary.FileWriter(\"logs/tf1/val\")\n",
    "test_writer= tf.summary.FileWriter(\"logs/tf1/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\t train loss=2.7197, train acc=0.1399\n",
      "\t valid loss=2.7430, valid acc=0.1187\n",
      "\t test loss=2.6952, test acc=0.1353\n",
      "Epoch 11/100\n",
      "\t train loss=1.4008, train acc=0.5685\n",
      "\t valid loss=1.4542, valid acc=0.5587\n",
      "\t test loss=1.4322, test acc=0.5553\n",
      "Epoch 21/100\n",
      "\t train loss=1.1427, train acc=0.6504\n",
      "\t valid loss=1.2151, valid acc=0.6300\n",
      "\t test loss=1.1929, test acc=0.6427\n",
      "Epoch 31/100\n",
      "\t train loss=0.9579, train acc=0.7066\n",
      "\t valid loss=1.0317, valid acc=0.6947\n",
      "\t test loss=1.0164, test acc=0.7000\n",
      "Epoch 41/100\n",
      "\t train loss=0.8629, train acc=0.7359\n",
      "\t valid loss=0.9426, valid acc=0.7227\n",
      "\t test loss=0.9361, test acc=0.7300\n",
      "Epoch 51/100\n",
      "\t train loss=0.7880, train acc=0.7604\n",
      "\t valid loss=0.8779, valid acc=0.7367\n",
      "\t test loss=0.8776, test acc=0.7460\n",
      "Epoch 61/100\n",
      "\t train loss=0.7304, train acc=0.7799\n",
      "\t valid loss=0.8227, valid acc=0.7487\n",
      "\t test loss=0.8334, test acc=0.7580\n",
      "Epoch 71/100\n",
      "\t train loss=0.6882, train acc=0.7916\n",
      "\t valid loss=0.7913, valid acc=0.7693\n",
      "\t test loss=0.7958, test acc=0.7707\n",
      "Epoch 81/100\n",
      "\t train loss=0.6577, train acc=0.7973\n",
      "\t valid loss=0.7665, valid acc=0.7740\n",
      "\t test loss=0.7729, test acc=0.7747\n",
      "Epoch 91/100\n",
      "\t train loss=0.6348, train acc=0.8012\n",
      "\t valid loss=0.7482, valid acc=0.7793\n",
      "\t test loss=0.7493, test acc=0.7780\n",
      "---------------------------------------------\n",
      "\n",
      "Test accuracy: 0.7813\n",
      "---------------------------------------------\n",
      "\n",
      "Model is saved to: ckpt/tf1/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "batch_size= 32\n",
    "history= []  #used to store train, valid accuracies and losses for showing later\n",
    "num_epoch = 100\n",
    "iter_per_epoch= math.ceil(float(train_size)/batch_size)  #number of iterations per epoch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init= tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver = tf.train.Saver() # Add ops to save and restore all the variables.\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        for idx_start in range(0, X_train.shape[0], batch_size):\n",
    "            idx_end = min(X_train.shape[0], idx_start + batch_size)\n",
    "            X_batch, y_batch = X_train[idx_start:idx_end], y_train[idx_start:idx_end]\n",
    "            sess.run([train_op], feed_dict={X:X_batch, y:y_batch})\n",
    "        #compute accuracies and losses at the end of each epoch\n",
    "        train_summary, train_loss, train_accuracy= sess.run([merged,loss, accuracy], feed_dict={X:X_train, y:y_train})\n",
    "        train_writer.add_summary(train_summary, epoch +1)\n",
    "        train_writer.flush()\n",
    "        \n",
    "        valid_summary, val_loss, val_accuracy= sess.run([merged,loss, accuracy], feed_dict={X:X_valid, y:y_valid})\n",
    "        valid_writer.add_summary(valid_summary, epoch +1)\n",
    "        valid_writer.flush()\n",
    "        \n",
    "        test_summary, test_loss, test_accuracy= sess.run([merged, loss, accuracy], feed_dict={X:X_test, y:y_test})\n",
    "        test_writer.add_summary(test_summary, epoch +1)\n",
    "        test_writer.flush()\n",
    "        \n",
    "        if epoch%10 == 0:\n",
    "            print(\"Epoch {}/{}\".format(epoch+1, num_epoch))\n",
    "            print(\"\\t train loss={:.4f}, train acc={:.4f}\".format(train_loss, train_accuracy))\n",
    "            print(\"\\t valid loss={:.4f}, valid acc={:.4f}\".format(val_loss, val_accuracy))\n",
    "            print(\"\\t test loss={:.4f}, test acc={:.4f}\".format(test_loss, test_accuracy))\n",
    "        \n",
    "        hist_item={\"train_loss\": train_loss, \"train_acc\": train_accuracy, \n",
    "                   \"val_loss\":val_loss, \"val_acc\": val_accuracy,\n",
    "                   \"test_loss\":test_loss, \"test_acc\": test_accuracy}\n",
    "        history.append(hist_item)\n",
    "        \n",
    "    print(\"---------------------------------------------\\n\")\n",
    "    test_accuracy= sess.run(accuracy, feed_dict={X:X_test, y:y_test})\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_accuracy))\n",
    "    \n",
    "    print(\"---------------------------------------------\\n\")\n",
    "    save_path = saver.save(sess, \"ckpt/tf1/model.ckpt\")\n",
    "    print(\"Model is saved to: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Open command line, nevigate to the folder of this tute and run **> tensorboard --logdir \"logs/tf1\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the train, valid accuracies and losses stored in the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\monash stuff\\education stuff\\3rd yr\\FIT3181 Deep Learning\\week 3\\tute 3\\Tutorial_03_with_answers\\FIT5215_Tut_03a_DNN_with_answers.ipynb Cell 79\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/monash%20stuff/education%20stuff/3rd%20yr/FIT3181%20Deep%20Learning/week%203/tute%203/Tutorial_03_with_answers/FIT5215_Tut_03a_DNN_with_answers.ipynb#Y131sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     plt\u001b[39m.\u001b[39mlegend()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/monash%20stuff/education%20stuff/3rd%20yr/FIT3181%20Deep%20Learning/week%203/tute%203/Tutorial_03_with_answers/FIT5215_Tut_03a_DNN_with_answers.ipynb#Y131sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/monash%20stuff/education%20stuff/3rd%20yr/FIT3181%20Deep%20Learning/week%203/tute%203/Tutorial_03_with_answers/FIT5215_Tut_03a_DNN_with_answers.ipynb#Y131sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m plot_history_v2(history)\n",
      "\u001b[1;32md:\\monash stuff\\education stuff\\3rd yr\\FIT3181 Deep Learning\\week 3\\tute 3\\Tutorial_03_with_answers\\FIT5215_Tut_03a_DNN_with_answers.ipynb Cell 79\u001b[0m in \u001b[0;36mplot_history_v2\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/monash%20stuff/education%20stuff/3rd%20yr/FIT3181%20Deep%20Learning/week%203/tute%203/Tutorial_03_with_answers/FIT5215_Tut_03a_DNN_with_answers.ipynb#Y131sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     test_losses\u001b[39m.\u001b[39mappend(h_item[\u001b[39m\"\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/monash%20stuff/education%20stuff/3rd%20yr/FIT3181%20Deep%20Learning/week%203/tute%203/Tutorial_03_with_answers/FIT5215_Tut_03a_DNN_with_answers.ipynb#Y131sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     test_accuracies\u001b[39m.\u001b[39mappend(h_item[\u001b[39m\"\u001b[39m\u001b[39mtest_acc\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/monash%20stuff/education%20stuff/3rd%20yr/FIT3181%20Deep%20Learning/week%203/tute%203/Tutorial_03_with_answers/FIT5215_Tut_03a_DNN_with_answers.ipynb#Y131sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/monash%20stuff/education%20stuff/3rd%20yr/FIT3181%20Deep%20Learning/week%203/tute%203/Tutorial_03_with_answers/FIT5215_Tut_03a_DNN_with_answers.ipynb#Y131sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(train_losses, \u001b[39m\"\u001b[39m\u001b[39mr.-\u001b[39m\u001b[39m\"\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain loss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/monash%20stuff/education%20stuff/3rd%20yr/FIT3181%20Deep%20Learning/week%203/tute%203/Tutorial_03_with_answers/FIT5215_Tut_03a_DNN_with_answers.ipynb#Y131sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(valid_losses, \u001b[39m\"\u001b[39m\u001b[39mb.-\u001b[39m\u001b[39m\"\u001b[39m, label\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalid loss\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_history_v2(history):\n",
    "    train_losses =[]\n",
    "    train_accuracies=[]\n",
    "    valid_losses=[]\n",
    "    valid_accuracies=[]\n",
    "    test_losses=[]\n",
    "    test_accuracies=[]\n",
    "    for h_item in history:\n",
    "        train_losses.append(h_item[\"train_loss\"])\n",
    "        train_accuracies.append(h_item[\"train_acc\"])\n",
    "        valid_losses.append(h_item[\"val_loss\"])\n",
    "        valid_accuracies.append(h_item[\"val_acc\"])\n",
    "        test_losses.append(h_item[\"test_loss\"])\n",
    "        test_accuracies.append(h_item[\"test_acc\"])\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(train_losses, \"r.-\", label=\"train loss\")\n",
    "    plt.plot(valid_losses, \"b.-\", label= \"valid loss\")\n",
    "    plt.plot(test_losses, \"y.-\", label= \"test loss\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(train_accuracies, \"r.-\", label=\"train acc\")\n",
    "    plt.plot(valid_accuracies, \"b.-\", label= \"valid acc\")\n",
    "    plt.plot(test_accuracies, \"y.-\", label= \"test acc\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_history_v2(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore the saved model and predict some samples in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input name:  network/Placeholder:0\n",
      "Output name:  network/logits/add:0\n"
     ]
    }
   ],
   "source": [
    "# We need to get tensor names of input X and output logits\n",
    "X_name = X.name\n",
    "logits_name = logits.name\n",
    "print(\"Input name: \", X.name)\n",
    "print(\"Output name: \", logits.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/tf1/model.ckpt\n",
      "[[ -8.815776   -2.3259957  -9.119989    2.246305  -18.197033    1.6822797\n",
      "   -9.59643    -7.906055   -4.053893   -3.5611804 -15.04044   -20.608627\n",
      "  -27.016129   -8.691115   -7.9207067   4.3958683 -23.262846   -6.2989106\n",
      "   -6.9420476  -6.3497896 -19.89806   -23.333694  -65.56244   -12.400206\n",
      "  -17.81368   -22.071993 ]]\n",
      "Model predicts:  15\n",
      "---------------------------------------------\n",
      "\n",
      "[[-13.387064   -33.33628      0.34920788 -11.653497    -2.8754768\n",
      "  -62.82682      2.8226595  -10.16773     -6.325246    -2.9746988\n",
      "  -21.992485   -19.425297   -66.02736    -36.54733     -3.4931486\n",
      "   -6.382491    -1.8177103  -31.292831    -5.7363033   -8.4673395\n",
      "  -16.583462   -44.25059    -89.67516    -22.697538   -23.513239\n",
      "  -22.506298  ]]\n",
      "Model predicts:  6\n",
      "---------------------------------------------\n",
      "\n",
      "[[ -5.439631    -2.101554    -5.1249895   -5.1688547    1.0334721\n",
      "   -2.3085215   -1.6108465   -9.015166     1.5735028   -0.79963344\n",
      "  -10.96281     -1.6841121  -43.32328    -26.889376   -10.836818\n",
      "   -5.099827    -1.5203431   -6.7264256    3.3270328   -5.6833663\n",
      "  -22.05981    -26.37656    -86.96054     -0.2212975  -13.037896\n",
      "    2.6284037 ]]\n",
      "Model predicts:  18\n",
      "---------------------------------------------\n",
      "\n",
      "[[  3.4448452 -15.592588  -10.026711   -1.6041555 -14.653181  -18.556973\n",
      "   -3.1609578  -2.5684438  -6.9649687  -2.1854968  -3.4161453  -1.8808169\n",
      "   -9.242593   -6.6435075  -4.5257196  -8.366905   -0.9284402  -1.0257365\n",
      "   -2.4085789 -10.877091  -10.927334  -38.647594  -40.125275   -7.4694843\n",
      "  -17.933376   -9.110462 ]]\n",
      "Model predicts:  0\n",
      "---------------------------------------------\n",
      "\n",
      "[[-20.785984   -31.568504    -6.4771566   -5.1841874  -18.019344\n",
      "   -3.5746825  -14.302063    -3.322118    -5.661539    -7.9500585\n",
      "   -1.9738503   -4.7501493   -3.318841     2.932227    -8.118917\n",
      "  -12.210136   -13.171571   -14.879441   -13.9632225   -2.5198348\n",
      "    4.0473247    1.0236627    1.2399712   -1.5048578   -0.67637527\n",
      "  -22.644417  ]]\n",
      "Model predicts:  20\n",
      "---------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assume that we only have the saved model to deploy and predict each sample at a time.\n",
    "\n",
    "tf.reset_default_graph() # Clear all the default graph stack and resets the global default graph.\n",
    "    \n",
    "saver = tf.train.import_meta_graph('ckpt/tf1/model.ckpt.meta') # Load computational graph\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"ckpt/tf1/model.ckpt\") # Load saved weights\n",
    "    \n",
    "    # Get default graph (supply your custom graph if you have one)\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    # It will give tensor object\n",
    "    X = graph.get_tensor_by_name('network/Placeholder:0')\n",
    "    logits = graph.get_tensor_by_name('network/logits/add:0')\n",
    "      \n",
    "    # We predict first 5 samples\n",
    "    for x in X_test[:5]:        \n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        \n",
    "        output = sess.run(logits, feed_dict={X:x})\n",
    "        predicted = np.argmax(output)\n",
    "        \n",
    "        print(output)\n",
    "        print(\"Model predicts: \", predicted)\n",
    "        print(\"---------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#FFA500\"> Solution for exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X data shape: (20640, 8)\n",
      "y data shape: (20640, 1)\n",
      "x-min=-124.35, x-max=39320.0\n",
      "We need to scale the features of this data into [-1,1]\n"
     ]
    }
   ],
   "source": [
    "# We load and process the dataset\n",
    "data_file_name= \"cadata.libsvm\"\n",
    "data_file = os.path.abspath(\"./data/\" + data_file_name)\n",
    "X_data, y_data = load_svmlight_file(data_file)\n",
    "X_data= X_data.toarray()\n",
    "y_data= y_data.reshape(y_data.shape[0],-1)\n",
    "print(\"X data shape: {}\".format(X_data.shape))\n",
    "print(\"y data shape: {}\".format(y_data.shape))\n",
    "print(\"x-min={}, x-max={}\".format(np.min(X_data), np.max(X_data)))\n",
    "print(\"We need to scale the features of this data into [-1,1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-min=-1.0, x-max=1.0\n"
     ]
    }
   ],
   "source": [
    "# We scale the features of this data into [-1,1]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_data= MinMaxScaler(feature_range= (-1,1)).fit_transform(X_data)\n",
    "print(\"x-min={}, x-max={}\".format(np.min(X_data), np.max(X_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling: y-min =14999.0, y-max =500001.0\n",
      "After scaling: y-min =-0.9999999999999999, y-max =1.0\n",
      "Next step is to split the dataset into train set (80%), validation set (10%), and test set (10%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before scaling: y-min ={}, y-max ={}\".format(np.min(y_data), np.max(y_data)))\n",
    "y_data= MinMaxScaler(feature_range= (-1,1)).fit_transform(y_data)\n",
    "print(\"After scaling: y-min ={}, y-max ={}\".format(np.min(y_data), np.max(y_data)))\n",
    "print(\"Next step is to split the dataset into train set (80%), validation set (10%), and test set (10%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 8) (2064, 8) (2064, 8)\n",
      "(16512,) (2064,) (2064,)\n",
      "Three sets are ready! Next step is to build up a deep neural network.\n"
     ]
    }
   ],
   "source": [
    "# We split train, valid and test data\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = train_valid_test_split(X_data, y_data, train_size=0.8, test_size=0.1)\n",
    "y_train= y_train.reshape(-1)\n",
    "y_test= y_test.reshape(-1)\n",
    "y_valid= y_valid.reshape(-1)\n",
    "print(X_train.shape, X_valid.shape, X_test.shape)\n",
    "print(y_train.shape, y_valid.shape, y_test.shape)\n",
    "print(\"Three sets are ready! Next step is to build up a deep neural network.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now construct our network\n"
     ]
    }
   ],
   "source": [
    "train_size= X_train.shape[0]\n",
    "n_in= X_train.shape[1]\n",
    "print(\"We now construct our network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope(\"network\"):\n",
    "    X= tf.placeholder(shape=[None, n_in], dtype= tf.float32)\n",
    "    y= tf.placeholder(shape=[None], dtype= tf.float32)\n",
    "    logits= dense_layer(X, 1, name=\"logits\") # output has only one neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    MSE = tf.reduce_mean(tf.square(logits - y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "    train_op = optimizer.minimize(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: train_loss: 0.22929111123085022 - valid_loss: 0.23800340294837952\n",
      "Epoch 2/50: train_loss: 0.22613705694675446 - valid_loss: 0.23466163873672485\n",
      "Epoch 3/50: train_loss: 0.22482436895370483 - valid_loss: 0.23333440721035004\n",
      "Epoch 4/50: train_loss: 0.22434204816818237 - valid_loss: 0.2328415811061859\n",
      "Epoch 5/50: train_loss: 0.22418415546417236 - valid_loss: 0.2326733022928238\n",
      "Epoch 6/50: train_loss: 0.2241320163011551 - valid_loss: 0.2326134443283081\n",
      "Epoch 7/50: train_loss: 0.22411015629768372 - valid_loss: 0.2325865775346756\n",
      "Epoch 8/50: train_loss: 0.2240973860025406 - valid_loss: 0.23257064819335938\n",
      "Epoch 9/50: train_loss: 0.2240884006023407 - valid_loss: 0.23255962133407593\n",
      "Epoch 10/50: train_loss: 0.22408168017864227 - valid_loss: 0.2325514554977417\n",
      "Epoch 11/50: train_loss: 0.2240765243768692 - valid_loss: 0.23254530131816864\n",
      "Epoch 12/50: train_loss: 0.22407260537147522 - valid_loss: 0.23254065215587616\n",
      "Epoch 13/50: train_loss: 0.22406965494155884 - valid_loss: 0.23253720998764038\n",
      "Epoch 14/50: train_loss: 0.22406750917434692 - valid_loss: 0.23253469169139862\n",
      "Epoch 15/50: train_loss: 0.22406597435474396 - valid_loss: 0.23253296315670013\n",
      "Epoch 16/50: train_loss: 0.2240649163722992 - valid_loss: 0.2325318157672882\n",
      "Epoch 17/50: train_loss: 0.22406423091888428 - valid_loss: 0.2325311303138733\n",
      "Epoch 18/50: train_loss: 0.22406382858753204 - valid_loss: 0.2325308471918106\n",
      "Epoch 19/50: train_loss: 0.22406363487243652 - valid_loss: 0.23253081738948822\n",
      "Epoch 20/50: train_loss: 0.22406360507011414 - valid_loss: 0.23253102600574493\n",
      "Epoch 21/50: train_loss: 0.22406363487243652 - valid_loss: 0.2325313538312912\n",
      "Epoch 22/50: train_loss: 0.22406375408172607 - valid_loss: 0.23253180086612701\n",
      "Epoch 23/50: train_loss: 0.224063903093338 - valid_loss: 0.23253227770328522\n",
      "Epoch 24/50: train_loss: 0.22406402230262756 - valid_loss: 0.2325327843427658\n",
      "Epoch 25/50: train_loss: 0.2240641713142395 - valid_loss: 0.23253335058689117\n",
      "Epoch 26/50: train_loss: 0.22406423091888428 - valid_loss: 0.23253387212753296\n",
      "Epoch 27/50: train_loss: 0.22406424582004547 - valid_loss: 0.23253431916236877\n",
      "Epoch 28/50: train_loss: 0.22406424582004547 - valid_loss: 0.2325347512960434\n",
      "Epoch 29/50: train_loss: 0.2240641564130783 - valid_loss: 0.23253513872623444\n",
      "Epoch 30/50: train_loss: 0.22406400740146637 - valid_loss: 0.23253542184829712\n",
      "Epoch 31/50: train_loss: 0.22406378388404846 - valid_loss: 0.2325356900691986\n",
      "Epoch 32/50: train_loss: 0.22406353056430817 - valid_loss: 0.23253588378429413\n",
      "Epoch 33/50: train_loss: 0.22406315803527832 - valid_loss: 0.23253601789474487\n",
      "Epoch 34/50: train_loss: 0.2240627259016037 - valid_loss: 0.23253607749938965\n",
      "Epoch 35/50: train_loss: 0.2240622490644455 - valid_loss: 0.23253604769706726\n",
      "Epoch 36/50: train_loss: 0.2240617275238037 - valid_loss: 0.23253598809242249\n",
      "Epoch 37/50: train_loss: 0.22406111657619476 - valid_loss: 0.23253583908081055\n",
      "Epoch 38/50: train_loss: 0.22406046092510223 - valid_loss: 0.23253563046455383\n",
      "Epoch 39/50: train_loss: 0.22405977547168732 - valid_loss: 0.23253539204597473\n",
      "Epoch 40/50: train_loss: 0.22405901551246643 - valid_loss: 0.23253507912158966\n",
      "Epoch 41/50: train_loss: 0.22405821084976196 - valid_loss: 0.232534721493721\n",
      "Epoch 42/50: train_loss: 0.2240574061870575 - valid_loss: 0.23253434896469116\n",
      "Epoch 43/50: train_loss: 0.22405651211738586 - valid_loss: 0.23253388702869415\n",
      "Epoch 44/50: train_loss: 0.22405563294887543 - valid_loss: 0.23253339529037476\n",
      "Epoch 45/50: train_loss: 0.2240547090768814 - valid_loss: 0.23253284394741058\n",
      "Epoch 46/50: train_loss: 0.22405372560024261 - valid_loss: 0.2325323075056076\n",
      "Epoch 47/50: train_loss: 0.22405274212360382 - valid_loss: 0.23253171145915985\n",
      "Epoch 48/50: train_loss: 0.22405172884464264 - valid_loss: 0.2325311005115509\n",
      "Epoch 49/50: train_loss: 0.22405071556568146 - valid_loss: 0.23253044486045837\n",
      "Epoch 50/50: train_loss: 0.2240496575832367 - valid_loss: 0.23252978920936584\n",
      "---------------------------------------------\n",
      "\n",
      "Test MSE = 0.24193482100963593\n"
     ]
    }
   ],
   "source": [
    "batch_size= 32\n",
    "num_epoch = 50\n",
    "with tf.Session() as sess:\n",
    "    init= tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epoch):\n",
    "        for idx_start in range(0, X_train.shape[0], batch_size):\n",
    "            idx_end = min(X_train.shape[0], idx_start + batch_size)\n",
    "            X_batch, y_batch = X_train[idx_start:idx_end], y_train[idx_start:idx_end]\n",
    "            sess.run([train_op], feed_dict={X:X_batch, y:y_batch})\n",
    "        # compute MSE at the end of each epoch\n",
    "        train_MSE= sess.run(MSE, feed_dict={X:X_train, y:y_train})\n",
    "        val_MSE= sess.run(MSE, feed_dict={X:X_valid, y:y_valid})\n",
    "        print(\"Epoch {}/{}: train_loss: {} - valid_loss: {}\".format(epoch+1, num_epoch, train_MSE, val_MSE))\n",
    "        \n",
    "    print(\"---------------------------------------------\\n\")\n",
    "    test_MSE= sess.run(MSE, feed_dict={X:X_test, y:y_test})\n",
    "    print(\"Test MSE = {}\".format(test_MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf2_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "92c79073133677da89801d1b4bc42714b1a3d83cbc90a7f9c522b094b613b522"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
