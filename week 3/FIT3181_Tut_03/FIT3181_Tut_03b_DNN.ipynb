{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (2022)</span>\n",
    "***\n",
    "*CE/Lecturer (Clayton):*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Lecturer (Malaysia):*  **Dr Lim Chern Hong** | lim.chernhong@monash.edu <br/>  <br/>\n",
    "*Tutor:*  **Mr Thanh Nguyen** \\[Thanh.Nguyen4@monash.edu \\] |**Mr Tuan Nguyen**  \\[tuan.ng@monash.edu \\] |**Mr Anh Bui** \\[tuananh.bui@monash.edu\\] | **Dr Binh Nguyen** \\[binh.nguyen1@monash.edu \\] | **Mr Md Mohaimenuzzaman** \\[md.mohaimen@monash.edu \\] |**Mr James Tong** \\[james.tong1@monash.edu \\]\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Tutorial 3b: Feed-forward Neural Nets with TensorFlow 2.x</span>\n",
    "**This continues Tutorial 2a and shows you how to implement a feedforward neural network using TF 2.x**:  \n",
    "- ***Inspect how to use keras in TF 2.x to fulfill the task. As you can see later, the implementation is much simpler*.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> II.1 Feedforward Neural Network </span> <span style=\"color:red\">***** (highly important)</span>\n",
    "#### <span style=\"color:#0b486b\"> Tutorial objective </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will consider a fairly realistic deep NNs with *three* layers plus the *output* layer. Its architecture will be specified as: $16 \\rightarrow 10 (ReLU) \\rightarrow 20 (ReLU) \\rightarrow 15 (ReLu) \\rightarrow 26$. This means:\n",
    "- Input size is 16\n",
    "- First layer has 10 hidden units with ReLU activation function\n",
    "- Second layer has 20 hidden units with 20 ReLU activiation function\n",
    "- Third layer has 15 hidden units with 15 ReLU activiation function\n",
    "- And output layer is logit layer with 26 hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network, for example, can take the `letter` dataset input with $16$ features and with $26$ classes (A-Z). **Our objective in this tutorial is to implement this specific network in `TensorFlow 2.x`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> II.2 Implementation with TensorFlow 2.x</span> <span style=\"color:red\">***** (highly important)</span>\n",
    "We now shall implement the aforementioned network with the architecture of $16 \\rightarrow 10 (ReLU) \\rightarrow 20 (ReLU) \\rightarrow 15 (ReLu) \\rightarrow 26$ in Tensorflow using the dataset `letter`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This letter dataset can be found at [the LIBSVM website](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#letter). Here is the dataset information:\n",
    "-  *The objective is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical pipeline process of implementing a deep learning model is as follows:\n",
    "\n",
    "1. **Data processing**: \n",
    "    - Load the dataset and split into train, valid, and test sets.  \n",
    "     \n",
    "2. **Building the model**: \n",
    "    - Build the model using keras layers.\n",
    "     \n",
    "3. **Compiling the model**: \n",
    "    - Compile the model and specify the optimizer, the loss (e.g., cross-entropy loss) you want to optimize, metrics you want to measure. \n",
    "    \n",
    "4. **Training and evalutating**:\n",
    "    - Train the model with specific training set and validation set in a number of epochs.\n",
    "    - Predict on the test set and assess its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">1. Data Processing </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `sklearn` to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X data shape: (15000, 16)\n",
      "y data shape: (15000, 1)\n",
      "# classes: 26\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      " 19. 20. 21. 22. 23. 24. 25. 26.]\n"
     ]
    }
   ],
   "source": [
    "data_file_name= \"letter_scale.libsvm\"\n",
    "data_file = os.path.abspath(\"./Data/\" + data_file_name)\n",
    "X_data, y_data = load_svmlight_file(data_file)\n",
    "X_data= X_data.toarray()\n",
    "y_data= y_data.reshape(y_data.shape[0],-1)\n",
    "print(\"X data shape: {}\".format(X_data.shape))\n",
    "print(\"y data shape: {}\".format(y_data.shape))\n",
    "print(\"# classes: {}\".format(len(np.unique(y_data))))\n",
    "print(np.unique(y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `sklearn` to split the dataset into the train, validation, and test sets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def train_valid_test_split(data, target, train_size, test_size):\n",
    "    valid_size = 1 - (train_size + test_size)\n",
    "    X1, X_test, y1, y_test = train_test_split(data, target, test_size = test_size, random_state= 33)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X1, y1, test_size = float(valid_size)/(valid_size+ train_size))\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would like to encode the label in the form of numeric vector. For example, we want to turn $y\\_data=[\"cat\", \"dog\", \"cat\", \"lion\", \"dog\"]$ to $y\\_data=[0,1,0,2,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, in the following segment of code, we use the object `le` as an instance of the class `preprocessing.LabelEncoder()` which supports us to transform catefgorial labels in `y_data` to numerical vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 15 18 ...  0 11 21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\tf2_cpu\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_data.ravel())\n",
    "y_data= le.transform(y_data)\n",
    "y_data = y_data.ravel()\n",
    "print(y_data[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the function defined above to prepare our data for training, validating and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 16) (1500, 16) (1500, 16)\n",
      "(12000,) (1500,) (1500,)\n",
      "lables: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test = train_valid_test_split(X_data, y_data, \n",
    "                                                                            train_size=0.8, \n",
    "                                                                            test_size=0.1)\n",
    "y_train= y_train.reshape(-1)\n",
    "y_test= y_test.reshape(-1)\n",
    "y_valid= y_valid.reshape(-1)\n",
    "print(X_train.shape, X_valid.shape, X_test.shape)\n",
    "print(y_train.shape, y_valid.shape, y_test.shape)\n",
    "print(\"lables: {}\".format(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size= int(X_train.shape[0])\n",
    "n_features= int(X_train.shape[1])\n",
    "n_classes= len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">2. Build up the model </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build up a feedforward neural network with the architecture: $16 \\rightarrow 10 (ReLU) \\rightarrow 20 (ReLU) \\rightarrow 15 (ReLu) \\rightarrow 26$ in TensorFlow 2.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = Sequential()  \n",
    "# sequetial is different \n",
    "#from sequential architectures that deal with temporal data\n",
    "\n",
    "# these are the DNN layers\n",
    "dnn_model.add(Dense(units=10,  input_shape=(16,), activation='relu'))\n",
    "dnn_model.add(Dense(units=20, activation='relu'))\n",
    "dnn_model.add(Dense(units=15, activation='relu'))\n",
    "dnn_model.add(Dense(units=n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                170       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 26)                416       \n",
      "=================================================================\n",
      "Total params: 1,121\n",
      "Trainable params: 1,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnn_model.build()\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Dense at 0x19abcad7430>,\n",
       " <keras.layers.core.Dense at 0x19abcad7490>,\n",
       " <keras.layers.core.Dense at 0x19abcb1b850>,\n",
       " <keras.layers.core.Dense at 0x19abcc0e430>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense\n"
     ]
    }
   ],
   "source": [
    "hidden1 = dnn_model.layers[0]\n",
    "hidden1\n",
    "print(hidden1.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">3. Compiling Model </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model.compile(optimizer='adam', \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">4. Training and Evaluating </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:#0b486b\"> Visualizing Training Progress </span>\n",
    "In this example, we demonstrate two approaches to visualize training progress, using a History object and using TensorBoard.\n",
    "\n",
    "**Using History object** \n",
    "The history object is the output of `fit()` method, which contains the training parameters (history.params), the list of epochs it went through (history.epoch), and most importantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if any). The training need to be finished before we can visualize using the history output. \n",
    "\n",
    "**Using TensorBoard**\n",
    "To visualize with TensorBoard we first need to create a `tensorboard callback` method with specific log directory. We then pass the callback method to `model.fit()` method. Unlike the previous method, the callback method writes log data to the log file on-the-fly. Therefore, by opening Tensorboard on a separate browser, we can train a model and parallelly visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "375/375 [==============================] - 5s 7ms/step - loss: 2.9121 - accuracy: 0.1538 - val_loss: 2.2002 - val_accuracy: 0.3600\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.8010 - accuracy: 0.4723 - val_loss: 1.5667 - val_accuracy: 0.5533\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.4741 - accuracy: 0.5726 - val_loss: 1.4030 - val_accuracy: 0.6127\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.3445 - accuracy: 0.6198 - val_loss: 1.3102 - val_accuracy: 0.6480\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.2579 - accuracy: 0.6457 - val_loss: 1.2489 - val_accuracy: 0.6580\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.1972 - accuracy: 0.6622 - val_loss: 1.2097 - val_accuracy: 0.6620\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.1482 - accuracy: 0.6742 - val_loss: 1.1744 - val_accuracy: 0.6813\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.1071 - accuracy: 0.6868 - val_loss: 1.1415 - val_accuracy: 0.6947\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.0744 - accuracy: 0.6961 - val_loss: 1.1105 - val_accuracy: 0.7020\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.0465 - accuracy: 0.7038 - val_loss: 1.0934 - val_accuracy: 0.6953\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.0204 - accuracy: 0.7122 - val_loss: 1.0632 - val_accuracy: 0.7067\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9989 - accuracy: 0.7178 - val_loss: 1.0525 - val_accuracy: 0.7147\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9771 - accuracy: 0.7202 - val_loss: 1.0319 - val_accuracy: 0.7127\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9572 - accuracy: 0.7258 - val_loss: 1.0199 - val_accuracy: 0.7087\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9432 - accuracy: 0.7296 - val_loss: 1.0089 - val_accuracy: 0.7173\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9268 - accuracy: 0.7349 - val_loss: 0.9903 - val_accuracy: 0.7200\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9109 - accuracy: 0.7400 - val_loss: 0.9754 - val_accuracy: 0.7220\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8978 - accuracy: 0.7412 - val_loss: 0.9533 - val_accuracy: 0.7327\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8847 - accuracy: 0.7408 - val_loss: 0.9437 - val_accuracy: 0.7300\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8721 - accuracy: 0.7502 - val_loss: 0.9348 - val_accuracy: 0.7333\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "logdir = \"tf_logs/\"\n",
    "\n",
    "# Init a tensorboard_callback \n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Call the fit method, passing the tensorboard_callback \n",
    "# ME: before this step, we already constructed the layers\n",
    "history = dnn_model.fit(x=X_train, y=y_train, batch_size=32, \n",
    "                        epochs=20, \n",
    "                        validation_data=(X_valid, y_valid), \n",
    "                        callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can evaluate the trained model on the testing set or any subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 1ms/step - loss: 0.9198 - accuracy: 0.7407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.919770359992981, 0.7406666874885559]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model.evaluate(X_test, y_test)  #return loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.93, 0.04, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  ,\n",
       "        0.01, 0.  , 0.  , 0.  ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.reshape(X_test[10, :], (1,-1))\n",
    "y_prob = dnn_model.predict(X_new)\n",
    "y_prob.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect prediction !\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(dnn_model.predict(X_new), axis=-1)\n",
    "if y_pred[0]==y_test[0]:\n",
    "    print(\"Correct predeiction !\")\n",
    "else:\n",
    "    print(\"Incorrect prediction !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">5. Visualizing the Performance and Loss Objective Function </span>\n",
    "\n",
    "The `fit()` method returns a History object containing the training parameters (history.params), the list of epochs it went through (history.epoch), and most importantly a dictionary (history.history) containing the loss (`sparse_categorical_crossentropy`) and extra metrics (`accuracy`) as setted when compiling model.\n",
    "There are four keys in the history dictionary: `loss` and `val_loss` measure the loss on the training set and the validation set, respectively, while `accuracy` and `val_accuracy` measure the accuracy on the training set and the validation set.  \n",
    "The following figure visualize all four metrics with two y-axes, losses (blue lines, in descending) and accuracies (red lines, in asending) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\fit3181 tutes\\week 3\\FIT3181_Tut_03\\FIT3181_Tut_03b_DNN.ipynb Cell 44\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/fit3181%20tutes/week%203/FIT3181_Tut_03/FIT3181_Tut_03b_DNN.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/fit3181%20tutes/week%203/FIT3181_Tut_03/FIT3181_Tut_03b_DNN.ipynb#X61sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/fit3181%20tutes/week%203/FIT3181_Tut_03/FIT3181_Tut_03b_DNN.ipynb#X61sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m his \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "his = history.history \n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ln1 = ax.plot(his['loss'], 'b--',label='loss')\n",
    "ln2 = ax.plot(his['val_loss'], 'b-',label='val_loss')\n",
    "ax.set_ylabel('loss', color='blue')\n",
    "ax.tick_params(axis='y', colors=\"blue\")\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ln3 = ax2.plot(his['accuracy'], 'r--',label='accuracy')\n",
    "ln4 = ax2.plot(his['val_accuracy'], 'r-',label='val_accuracy')\n",
    "ax2.set_ylabel('accuracy', color='red')\n",
    "ax2.tick_params(axis='y', colors=\"red\")\n",
    "\n",
    "\n",
    "lns = ln1 + ln2 + ln3 + ln4 \n",
    "labels = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labels)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize using Tensorboard on the same jupyter notebook, we first need to load the TensorBoard extension. Then just calling the tensorboard with log file directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 11564."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tf_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">6. Playing around with different optimizers</span>\n",
    "In the following code we will try different optimizers to find which has the best performance (evaluate on the validation set). \n",
    "It can be done easily by passing an specific optimizer when compiling model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Evaluating with Nadam\n",
      "\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 0.7491 - accuracy: 0.7680\n",
      "The valid accuracy is 0.7680000066757202\n",
      "\n",
      "*Evaluating with Adam\n",
      "\n",
      "1500/1500 [==============================] - 0s 19us/sample - loss: 0.6767 - accuracy: 0.7920\n",
      "The valid accuracy is 0.7919999957084656\n",
      "\n",
      "*Evaluating with Adadelta\n",
      "\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 0.6694 - accuracy: 0.7953\n",
      "The valid accuracy is 0.7953333258628845\n",
      "\n",
      "*Evaluating with Adagrad\n",
      "\n",
      "1500/1500 [==============================] - 0s 18us/sample - loss: 0.6641 - accuracy: 0.8007\n",
      "The valid accuracy is 0.8006666898727417\n",
      "\n",
      "*Evaluating with RMSprop\n",
      "\n",
      "1500/1500 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.7953\n",
      "The valid accuracy is 0.7953333258628845\n",
      "\n",
      "*Evaluating with SGD\n",
      "\n",
      "1500/1500 [==============================] - 0s 17us/sample - loss: 0.6213 - accuracy: 0.8040\n",
      "The valid accuracy is 0.8040000200271606\n",
      "\n",
      "The best valid accuracy is 0.8040000200271606 with SGD\n"
     ]
    }
   ],
   "source": [
    "optimizer_names = [\"Nadam\", \"Adam\", \"Adadelta\", \"Adagrad\", \"RMSprop\", \"SGD\"]\n",
    "optimizer_list = [keras.optimizers.Nadam(learning_rate=0.001), keras.optimizers.Adam(learning_rate=0.001), keras.optimizers.Adadelta(learning_rate=0.001), \n",
    "                  keras.optimizers.Adagrad(learning_rate=0.001), keras.optimizers.RMSprop(learning_rate=0.001), keras.optimizers.SGD(learning_rate=0.001)]\n",
    "best_acc = 0\n",
    "best_i = -1\n",
    "for i in range(len(optimizer_list)):\n",
    "    print(\"*Evaluating with {}\\n\".format(str(optimizer_names[i])))\n",
    "    dnn_model.compile(optimizer=optimizer_list[i], loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    dnn_model.fit(x=X_train, y=y_train, batch_size=32, epochs=30, validation_data=(X_valid, y_valid), verbose=0)\n",
    "    acc = dnn_model.evaluate(X_valid, y_valid)[1]\n",
    "    print(\"The valid accuracy is {}\\n\".format(acc))\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_i = i\n",
    "print(\"The best valid accuracy is {} with {}\".format(best_acc, optimizer_names[best_i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">7. Fine-tuning the learning rate</span>\n",
    "Learning rate plays an important role when training a deep learning model. In the following code, we will try a simple greedy search to find a good learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Evaluating with learning rate = 0.01\n",
      "\n",
      "1500/1500 [==============================] - 0s 18us/sample - loss: 0.5262 - accuracy: 0.8333\n",
      "The valid accuracy is 0.8333333134651184\n",
      "\n",
      "*Evaluating with learning rate = 0.005\n",
      "\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 0.4689 - accuracy: 0.8447\n",
      "The valid accuracy is 0.8446666598320007\n",
      "\n",
      "*Evaluating with learning rate = 0.001\n",
      "\n",
      "1500/1500 [==============================] - 0s 18us/sample - loss: 0.3890 - accuracy: 0.8733\n",
      "The valid accuracy is 0.8733333349227905\n",
      "\n",
      "*Evaluating with learning rate = 0.0001\n",
      "\n",
      "1500/1500 [==============================] - 0s 19us/sample - loss: 0.3902 - accuracy: 0.8713\n",
      "The valid accuracy is 0.8713333606719971\n",
      "\n",
      "*Evaluating with learning rate = 1e-05\n",
      "\n",
      "1500/1500 [==============================] - 0s 17us/sample - loss: 0.3898 - accuracy: 0.8727\n",
      "The valid accuracy is 0.8726666569709778\n",
      "\n",
      "The best valid accuracy is 0.8733333349227905 with learning rate 0.001\n"
     ]
    }
   ],
   "source": [
    "lr = [1e-2, 5e-3, 1e-3, 1e-4, 1e-5] # ME: a range of learning rate values\n",
    "\n",
    "best_acc = 0\n",
    "best_i = -1\n",
    "for i in range(len(lr)):\n",
    "    print(\"*Evaluating with learning rate = {}\\n\".format(str(lr[i])))\n",
    "    dnn_model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr[i]), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    dnn_model.fit(x=X_train, y=y_train, batch_size=32, epochs=30, validation_data=(X_valid, y_valid), verbose=0)\n",
    "    acc = dnn_model.evaluate(X_valid, y_valid)[1]\n",
    "    print(\"The valid accuracy is {}\\n\".format(acc))\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_i = i\n",
    "print(\"The best valid accuracy is {} with learning rate {}\".format(best_acc, lr[best_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">8. Save and Load Models</span>\n",
    "\n",
    "There are different ways to save TensorFlow models depending on the API you're using. As we used the Keras model in this tutorial, saving and loading it quite simple. It can be done by calling `model.save()` and `load_model()` methods. \n",
    "When calling `model.save()`, the entire model will be saved including: \n",
    "- The architecture, or configuration, which specifies what layers the model contain, and how they're connected.\n",
    "- A set of weights values (the \"state of the model\").\n",
    "- An optimizer (defined by compiling the model).\n",
    "- A set of losses and metrics (defined by compiling the model or calling add_loss() or add_metric())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\trung\\Anaconda3\\envs\\tf2x_cpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/assets\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True]]\n"
     ]
    }
   ],
   "source": [
    "# Saving the entire model to a directory\n",
    "dnn_model.save('models/')\n",
    "\n",
    "# Loading the model back \n",
    "from tensorflow import keras\n",
    "loaded_model = keras.models.load_model('models/')\n",
    "\n",
    "# Checking the loaded model \n",
    "print(dnn_model.predict(X_new) == loaded_model.predict(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Save model during training</span>\n",
    "\n",
    "One major disadvantage of the above saving method is that we cannot save the model during training but only when the training is finished. Therefore, it can be the case when the training was stopped/interrupted and we have to retrain again. To save model during training process, we can use the `ModelCheckpoint` callback allows you to continually save the model both during and at the end of training. \n",
    "Some important arguments of the `ModelCheckpoint` callback: \n",
    "- `filepath`: checkpoin directory \n",
    "- `save_weights_only`: if True, then only the model's weights will be saved (i.e., equal with model.save_weights(filepath)), else the full model is saved (i.e., equal with model.save(filepath) which saves: model' weight, model architecture, optimizer, etc.)\n",
    "- `save_best_only`: if True, it only saves when the model is considered the \"best\" and the latest best model according to the quantity monitored will not be overwritten. The \"best\" model is evaluated based on \"mode\" and \"monitor\". For example, if `monitor=val_accuracy` it means that validation accuracy is used to monitor the best checkpoint, and `mode` should be set to `max`. If `monitor=val_loss` it means that validation loss is used instead, and `mode` in this case should be `min`. \n",
    "\n",
    "More detail can be found in the link: \n",
    "https://www.tensorflow.org/tutorials/keras/save_and_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 1500 samples\n",
      "Epoch 1/20\n",
      "11616/12000 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.9074\n",
      "Epoch 00001: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 70us/sample - loss: 0.2986 - accuracy: 0.9066 - val_loss: 0.3897 - val_accuracy: 0.8727\n",
      "Epoch 2/20\n",
      "11648/12000 [============================>.] - ETA: 0s - loss: 0.3001 - accuracy: 0.9065\n",
      "Epoch 00002: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 59us/sample - loss: 0.2986 - accuracy: 0.9068 - val_loss: 0.3897 - val_accuracy: 0.8727\n",
      "Epoch 3/20\n",
      "10912/12000 [==========================>...] - ETA: 0s - loss: 0.2958 - accuracy: 0.9082\n",
      "Epoch 00003: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 57us/sample - loss: 0.2986 - accuracy: 0.9068 - val_loss: 0.3896 - val_accuracy: 0.8727\n",
      "Epoch 4/20\n",
      "11552/12000 [===========================>..] - ETA: 0s - loss: 0.2997 - accuracy: 0.9064\n",
      "Epoch 00004: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 55us/sample - loss: 0.2986 - accuracy: 0.9066 - val_loss: 0.3897 - val_accuracy: 0.8727\n",
      "Epoch 5/20\n",
      "11456/12000 [===========================>..] - ETA: 0s - loss: 0.2994 - accuracy: 0.9066 ETA: 0s - loss: 0.2992 - accuracy: 0.90\n",
      "Epoch 00005: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 53us/sample - loss: 0.2986 - accuracy: 0.9069 - val_loss: 0.3897 - val_accuracy: 0.8727\n",
      "Epoch 6/20\n",
      "11424/12000 [===========================>..] - ETA: 0s - loss: 0.2992 - accuracy: 0.9059\n",
      "Epoch 00006: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 53us/sample - loss: 0.2986 - accuracy: 0.9064 - val_loss: 0.3898 - val_accuracy: 0.8727\n",
      "Epoch 7/20\n",
      "11584/12000 [===========================>..] - ETA: 0s - loss: 0.2980 - accuracy: 0.9068\n",
      "Epoch 00007: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 54us/sample - loss: 0.2986 - accuracy: 0.9062 - val_loss: 0.3898 - val_accuracy: 0.8727\n",
      "Epoch 8/20\n",
      "10880/12000 [==========================>...] - ETA: 0s - loss: 0.3012 - accuracy: 0.9066\n",
      "Epoch 00008: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 85us/sample - loss: 0.2986 - accuracy: 0.9065 - val_loss: 0.3896 - val_accuracy: 0.8727\n",
      "Epoch 9/20\n",
      "11104/12000 [==========================>...] - ETA: 0s - loss: 0.2986 - accuracy: 0.9060\n",
      "Epoch 00009: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 53us/sample - loss: 0.2986 - accuracy: 0.9066 - val_loss: 0.3897 - val_accuracy: 0.8727\n",
      "Epoch 10/20\n",
      "11264/12000 [===========================>..] - ETA: 0s - loss: 0.2968 - accuracy: 0.9078\n",
      "Epoch 00010: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 56us/sample - loss: 0.2986 - accuracy: 0.9065 - val_loss: 0.3897 - val_accuracy: 0.8727\n",
      "Epoch 11/20\n",
      "11648/12000 [============================>.] - ETA: 0s - loss: 0.2989 - accuracy: 0.9067\n",
      "Epoch 00011: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 53us/sample - loss: 0.2986 - accuracy: 0.9065 - val_loss: 0.3897 - val_accuracy: 0.8727\n",
      "Epoch 12/20\n",
      "11488/12000 [===========================>..] - ETA: 0s - loss: 0.2981 - accuracy: 0.9065\n",
      "Epoch 00012: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 54us/sample - loss: 0.2986 - accuracy: 0.9064 - val_loss: 0.3898 - val_accuracy: 0.8727\n",
      "Epoch 13/20\n",
      "11296/12000 [===========================>..] - ETA: 0s - loss: 0.2990 - accuracy: 0.9066\n",
      "Epoch 00013: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 57us/sample - loss: 0.2986 - accuracy: 0.9068 - val_loss: 0.3899 - val_accuracy: 0.8727\n",
      "Epoch 14/20\n",
      "11104/12000 [==========================>...] - ETA: 0s - loss: 0.3004 - accuracy: 0.9054\n",
      "Epoch 00014: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 54us/sample - loss: 0.2986 - accuracy: 0.9066 - val_loss: 0.3899 - val_accuracy: 0.8727\n",
      "Epoch 15/20\n",
      "10592/12000 [=========================>....] - ETA: 0s - loss: 0.2956 - accuracy: 0.9074\n",
      "Epoch 00015: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 70us/sample - loss: 0.2986 - accuracy: 0.9067 - val_loss: 0.3897 - val_accuracy: 0.8727\n",
      "Epoch 16/20\n",
      "11424/12000 [===========================>..] - ETA: 0s - loss: 0.2962 - accuracy: 0.9070\n",
      "Epoch 00016: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 51us/sample - loss: 0.2986 - accuracy: 0.9067 - val_loss: 0.3898 - val_accuracy: 0.8727\n",
      "Epoch 17/20\n",
      "11456/12000 [===========================>..] - ETA: 0s - loss: 0.2990 - accuracy: 0.9069\n",
      "Epoch 00017: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 55us/sample - loss: 0.2986 - accuracy: 0.9068 - val_loss: 0.3897 - val_accuracy: 0.8727\n",
      "Epoch 18/20\n",
      "11200/12000 [===========================>..] - ETA: 0s - loss: 0.3010 - accuracy: 0.9053\n",
      "Epoch 00018: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 56us/sample - loss: 0.2986 - accuracy: 0.9067 - val_loss: 0.3899 - val_accuracy: 0.8727\n",
      "Epoch 19/20\n",
      "10784/12000 [=========================>....] - ETA: 0s - loss: 0.2985 - accuracy: 0.9068\n",
      "Epoch 00019: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 57us/sample - loss: 0.2986 - accuracy: 0.9066 - val_loss: 0.3897 - val_accuracy: 0.8727\n",
      "Epoch 20/20\n",
      "11584/12000 [===========================>..] - ETA: 0s - loss: 0.3012 - accuracy: 0.9059\n",
      "Epoch 00020: saving model to models/cp.ckpt\n",
      "12000/12000 [==============================] - 1s 54us/sample - loss: 0.2986 - accuracy: 0.9065 - val_loss: 0.3897 - val_accuracy: 0.8727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ac40d193c8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tf.keras.callbacks.ModelCheckpoint callback that saves weights only during training\n",
    "checkpoint_path = \"models/cp.ckpt\"  # saves weight during training\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "# ME: save_weights_only=True saves the weights only, next time we hae to use the same model, because it doent save the model\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Train the model with the new callback\n",
    "dnn_model.fit(x=X_train, y=y_train, batch_size=32, \n",
    "                        epochs=20, \n",
    "                        validation_data=(X_valid, y_valid), \n",
    "                       callbacks=[tensorboard_callback, # Callback for writing log \n",
    "                                 cp_callback]) # Callback for saving model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Save model during training</span>\n",
    "If we only saved the model's weight, we need to recreate a same architecture before loading the model weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2ac40d6f588>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because we alreary created a model, therefore, we just need to load the weight \n",
    "# dnn_model = create_model() # skip this step\n",
    "dnn_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we saved the entire model (by set `save_weights_only=False`), then the pretrained model can be reloaded by `load_model` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> II.3 Two Approaches to Build Up Models with TensorFlow 2.x</span> <span style=\"color:red\">*** (relatively important)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two approaches to build up a model with tensorflow 2.x, a simple method using **Sequential API** and a more flexible method using **Functional API**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\"> Approach 1: Using `Sequential API`</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = Sequential()\n",
    "dnn_model.add(Dense(units=10,  input_shape=(16,), activation='relu'))\n",
    "dnn_model.add(Dense(units=20, activation='relu'))\n",
    "dnn_model.add(Dense(units=15, activation='relu'))\n",
    "dnn_model.add(Dense(units=26, activation='softmax'))\n",
    "dnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\"> Approach 2: Using `Functional API`</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.layers.Input(shape=(16,)) #declare input layer\n",
    "h = Dense(units=10, activation= 'relu')(X)\n",
    "h = Dense(units=20, activation= 'relu')(h)\n",
    "h = Dense(units=15, activation= 'relu')(h)\n",
    "h = Dense(units=26, activation= 'softmax')(h)\n",
    "dnn_model = tf.keras.Model(inputs= X, outputs=h)\n",
    "dnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also declare a class inherited from `tf.keras.Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDNN(tf.keras.Model):\n",
    "    def __init__(self, n_classes= 26):\n",
    "        super(MyDNN, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.dense1 = tf.keras.layers.Dense(units=10, activation= 'relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(units=20, activation= 'relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(units=15, activation= 'relu')\n",
    "        self.dense4 = tf.keras.layers.Dense(units=self.n_classes, activation= 'softmax')\n",
    "    \n",
    "    def call(self,X): #X is the input, method call specifies how to compute the output from the input X\n",
    "        h = self.dense1(X)\n",
    "        h = self.dense2(h)\n",
    "        h = self.dense3(h)\n",
    "        h = self.dense4(h)\n",
    "        return h\n",
    "dnn_model = MyDNN(n_classes= 26)\n",
    "dnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> II.4 Other approaches to Train a Model with TensorFlow 2.x</span> <span style=\"color:red\">*** (relatively important)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main approaches to training a model with Tensorflow 2.x. The simplest method is the `fit` method as we did before. This method automatically helps us to process data when training (e.g., split an entire dataset into multiple mini-batches), applies callback methods such as saving model or writing TensorBoard and monitors validation performance. \n",
    "\n",
    "However, some projects require more handly on training process (for example, doing data augmentation in self-supervised learning or training a generative model that we will learn later in this unit). In this case, we need an ability/understanding to train a model manually. In Tensorflow 2.X, we can do that with `train_on_batch` method. Basically, we will need to *(1) manually split entire dataset into mini-batches and applied data augmentaion (if any)* and *(2) feed training data to `train_on_batch` method*. It returns a training loss (which is pre-defined when compiling model) and an updated model. \n",
    "\n",
    "The following code is a simple example (without any data-augmentation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train acc=0.1510, train loss=2.8678 | valid acc=0.1573, valid loss= 2.8721\n",
      "Epoch 2: train acc=0.3644, train loss=2.0786 | valid acc=0.3533, valid loss= 2.0877\n",
      "Epoch 3: train acc=0.5075, train loss=1.6448 | valid acc=0.4947, valid loss= 1.6496\n",
      "Epoch 4: train acc=0.5758, train loss=1.4298 | valid acc=0.5633, valid loss= 1.4345\n",
      "Epoch 5: train acc=0.6065, train loss=1.3327 | valid acc=0.5880, valid loss= 1.3390\n",
      "Epoch 6: train acc=0.6285, train loss=1.2727 | valid acc=0.6080, valid loss= 1.2805\n",
      "Epoch 7: train acc=0.6465, train loss=1.2245 | valid acc=0.6293, valid loss= 1.2329\n",
      "Epoch 8: train acc=0.6597, train loss=1.1807 | valid acc=0.6387, valid loss= 1.1892\n",
      "Epoch 9: train acc=0.6717, train loss=1.1417 | valid acc=0.6580, valid loss= 1.1502\n",
      "Epoch 10: train acc=0.6824, train loss=1.1079 | valid acc=0.6667, valid loss= 1.1171\n",
      "Epoch 11: train acc=0.6892, train loss=1.0780 | valid acc=0.6820, valid loss= 1.0883\n",
      "Epoch 12: train acc=0.6977, train loss=1.0516 | valid acc=0.6893, valid loss= 1.0622\n",
      "Epoch 13: train acc=0.7035, train loss=1.0282 | valid acc=0.6927, valid loss= 1.0394\n",
      "Epoch 14: train acc=0.7093, train loss=1.0080 | valid acc=0.6980, valid loss= 1.0197\n",
      "Epoch 15: train acc=0.7138, train loss=0.9897 | valid acc=0.7027, valid loss= 1.0017\n",
      "Epoch 16: train acc=0.7193, train loss=0.9732 | valid acc=0.7093, valid loss= 0.9852\n",
      "Epoch 17: train acc=0.7221, train loss=0.9582 | valid acc=0.7120, valid loss= 0.9700\n",
      "Epoch 18: train acc=0.7256, train loss=0.9448 | valid acc=0.7140, valid loss= 0.9563\n",
      "Epoch 19: train acc=0.7286, train loss=0.9320 | valid acc=0.7207, valid loss= 0.9436\n",
      "Epoch 20: train acc=0.7314, train loss=0.9204 | valid acc=0.7233, valid loss= 0.9325\n"
     ]
    }
   ],
   "source": [
    "n_epochs =20\n",
    "batch_size = 64\n",
    "for epoch in range(n_epochs):\n",
    "    for idx_start in range(0, X_train.shape[0], batch_size):\n",
    "        idx_end = min(X_train.shape[0], idx_start + batch_size)\n",
    "        X_batch, y_batch = X_train[idx_start:idx_end], y_train[idx_start:idx_end]\n",
    "        train_loss_batch = dnn_model.train_on_batch(X_batch, y_batch)  #return the batch loss\n",
    "        \n",
    "    train_loss, train_acc = dnn_model.evaluate(x= X_train, y= y_train, batch_size= 64, verbose= 0)\n",
    "    valid_loss, valid_acc = dnn_model.evaluate(x= X_valid, y= y_valid, batch_size= 64, verbose= 0)\n",
    "    print('Epoch {}: train acc={:.4f}, train loss={:.4f} | valid acc={:.4f}, valid loss= {:.4f}'.format(epoch +1, \n",
    "                                                                                                        train_acc, \n",
    "                                                                                                        train_loss, \n",
    "                                                                                                        valid_acc, \n",
    "                                                                                                        valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> Additional Exercises </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write your own code to save a trained model to the hard disk and restore this model, then use the restored model to output the prediction result on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Insert new code to the above code to enable outputting to TensorBoard the values of `training loss`, `training accuracy`, `valid loss`, and `valid accuracy` at the end of epochs. You can refer to the code [here](https://www.tensorflow.org/tensorboard/get_started)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write code to do regression on the dataset `cadata` which can be downloaded [here](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html). Note that for a regression problem, you need to use the `L2` loss instead of the `cross-entropy` loss as in a classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Using the problem in this tutorial, however, using a much deeper network (i.e., $16 \\rightarrow 100 (ReLU) \\rightarrow 200 (ReLU) \\rightarrow 200 (ReLU) \\rightarrow 100 (ReLu) \\rightarrow 26$). Applying callback methods to save the model on training and writing a TensorBoard. Visualize `training loss`, `training accuracy`, `valid loss`, and `valid accuracy`. Provide observation and explanation of any issue if happen (hint, overfitting issue). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Build up a more complex feedforward neural network with `Functional API` method  as shown in figure below. The network splits into two branches and then merges in the last layer. The concatenate operation is in the last dimenstion (for example, two arrays [10,15], [10,15] will be concatenated to an array [10,30]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/feed-forward-2branches.PNG\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf2_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "15d2ba40478b3ac6cd56796f62289265aeaa4b1060bcfd262a3fbfdb2f399e72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
