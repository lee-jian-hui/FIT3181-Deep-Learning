{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (2022)</span>\n",
    "***\n",
    "*CE/Lecturer (Clayton):*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Lecturer (Malaysia):*  **Dr Lim Chern Hong** | lim.chernhong@monash.edu <br/>  <br/>\n",
    "*Tutor:*  **Mr Thanh Nguyen** \\[Thanh.Nguyen4@monash.edu \\] |**Mr Tuan Nguyen**  \\[tuan.ng@monash.edu \\] |**Mr Anh Bui** \\[tuananh.bui@monash.edu\\] | **Dr Binh Nguyen** \\[binh.nguyen1@monash.edu \\] | **Mr Md Mohaimenuzzaman** \\[md.mohaimen@monash.edu \\] |**Mr James Tong** \\[james.tong1@monash.edu \\]\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***on Technology, Monash University, Australia\n",
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Tutorial 06a: Tricks of The Trade with TF 2.x</span><span style=\"color:red;  font-size: 18px\">***** (highly important)</span> #\n",
    "\n",
    "**The purpose of this tutorial is to demonstrate the important practical aspects of deep learning with TF 2.x. The following topics are presented:**\n",
    "1. How to apply weight initialization, batch normalization, and dropout to regularize deep networks and stabilize a training process.\n",
    "2. How to customize the loss function, weight initializer, and metric of interest for your models.\n",
    "3. How to do learning rate scheduler.\n",
    "4. How to monitor the training process to spot out and identify underfitting and overfitting and how to apply early stopping.\n",
    "5. How to reuse a pre-trained model and further do fine-tuning.\n",
    "\n",
    "***Acknowledgement***: *some content in this tutorial was developed based on the Chapter 12 materials from the book `Hands-on Machine Learning with Scikit-learn and Tensorflow`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">1. Download and prepare data</span> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the necessary packages and modules for this tutorial. Note that we import TF 2.x and keras which now becomes a part of TF 2.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, models, layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use dataset Cifar10 in this tutorial. Let's download and preprocess it by normalizing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000,)\n",
      "(10000, 32, 32, 3) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train) , (X_test, y_test) = datasets.cifar10.load_data()\n",
    "X_train, X_test = X_train/255.0, X_test/255.0\n",
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the training, we create the subsets of training and validation sets including 5,000 and 5,000 images respectively. When you run this tutorial on a stronger machine with GPU, you can try the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idxs = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(idxs)\n",
    "X_train = X_train[idxs]\n",
    "y_train = y_train[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (5000, 32, 32, 3) (5000,)\n",
      "Validation set (5000, 32, 32, 3) (5000,)\n"
     ]
    }
   ],
   "source": [
    "n_train, n_valid = 5000, 5000 \n",
    "X_train, X_valid = X_train[:n_train], X_train[n_train:n_train+n_valid]\n",
    "y_train, y_valid = y_train[:n_train], y_train[n_train:n_train+n_valid]\n",
    "print('Training set', X_train.shape, y_train.shape)\n",
    "print('Validation set', X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">2. Define a custom activation function, loss function, and metric</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">2.1. Define a custom activation function</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">2.1.1. Sigmoid activation function</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the *sigmoid activation function* as a TensorFlow function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + tf.math.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "plt.plot(z, sigmoid(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k--')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-5, 5], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.1, 1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the gradient of the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    x = tf.Variable(z)\n",
    "    y = sigmoid(x)\n",
    "grads = g.gradient(y,x)\n",
    "plt.plot(z, grads.numpy(), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-5, 5], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"Derivative of sigmoid\", fontsize=14)\n",
    "plt.axis([-5, 5, min(grads.numpy())- 1E-2, max(grads.numpy())+ 1E-2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">2.1.2. Tanh activation function</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the *tanh* activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return ((1-tf.math.exp(-2*z)) / (1 + tf.math.exp(-2*z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the *tanh* activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "plt.plot(z, tanh(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-5, 5], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"tanh activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.1, 1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the *gradient* of tanh activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    x = tf.Variable(z)\n",
    "    y = tanh(x)\n",
    "grads = g.gradient(y,x)\n",
    "plt.plot(z, grads.numpy(), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-5, 5], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"Derivative of tanh\", fontsize=14)\n",
    "plt.axis([-5, 5, min(grads.numpy())- 1E-2, max(grads.numpy()) + 1E-2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">2.1.3. Exponential linear unit (ELU) activation function</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the *ELU activation function* as a TensorFlow function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return tf.where(z < 0, alpha * (tf.math.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the ELU activation function as bellows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-5, 5], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the *gradient* of ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    x = tf.Variable(z)\n",
    "    y = elu(x)\n",
    "grads = g.gradient(y,x)\n",
    "plt.plot(z, grads.numpy(), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-5, 5], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"Derivative of ELU\", fontsize=14)\n",
    "plt.axis([-5, 5, min(grads.numpy())- 1E-2, max(grads.numpy()) + 1E-2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">2.1.4. ReLU (ReLU) activation function</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the *ReLU* activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return tf.maximum(0.0, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the *ReLU* activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: why we diont want to bound ReLu to 1, because we dont want to lose information, we dont want to suppress it into a limit value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "plt.plot(z, relu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-5, 5], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the *gradient* of ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    x = tf.Variable(z)\n",
    "    y = relu(x)\n",
    "grads = g.gradient(y,x)\n",
    "plt.plot(z, grads.numpy(), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-5, 5], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"Derivative of ReLU\", fontsize=14)\n",
    "plt.axis([-5, 5, min(grads.numpy())- 1E-2, max(grads.numpy()) + 1E-2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">2.1.5. Leaky ReLU (LeakyReLU) activation function</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is not 0 like ReLu at the bottom, sometimes wen't alwyas have 0\n",
    "- e.g. we want to divide, but we cant divide by 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the LeakyReLU activation function as a TF function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return tf.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the LeakyReLU activation function as a TF function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-5, 5], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the *gradient* of LeakyReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    x = tf.Variable(z)\n",
    "    y = leaky_relu(x, 0.05)\n",
    "grads = g.gradient(y,x)\n",
    "plt.plot(z, grads.numpy(), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-5, 5], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"Derivative of LeakyReLU\", fontsize=14)\n",
    "plt.axis([-5, 5, min(0, min(grads.numpy())) - 1E-2, max(grads.numpy()) + 1E-2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">2.1.6. SeLU  activation function (SeLU)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Anything that is more than 0 is still the same\n",
    "\n",
    "- anything less than 0 gets a bit complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the *SeLU* activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "# (see equation 14 in the paper):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 \n",
    "            + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * tf.where(z < 0, alpha * (tf.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot this activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "plt.plot(z, selu(z, scale=scale_0_1, alpha=alpha_0_1), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-5, 5], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    x = tf.Variable(z)\n",
    "    y = selu(x)\n",
    "grads = g.gradient(y,x)\n",
    "plt.plot(z, grads.numpy(), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-5, 5], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"Derivative of SELU\", fontsize=14)\n",
    "plt.axis([-5, 5, min(grads.numpy())- 1E-2, max(grads.numpy()) + 1E-2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:#0b486b\">2.2. Define a custom loss function</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define and customize your own loss function. In this example, we simply redefine the cross-entropy loss function. Note that to this end, we declare the class *my_CE_loss* which is inherited from the *keras.losses.Loss* class. All we need now are to override the method or function *call*. In particular, in this function, we have two important parameters: *y_true* and *y_pred*.\n",
    "- *y_true*: 1D tensor of true labels for the current mini-batch.\n",
    "- *y_pred*: 2D tensor of predicted probabilities with the shape of $batch\\_size \\times num\\_classes$.\n",
    "\n",
    "In addition, the *call* function has to return the 1D tensor of losses for the data examples in the current batch. You can refer to [this link](https://keras.io/api/losses/#creating-custom-losses) for more information regarding the existing loss function supported by Keras Tensorflow. \n",
    "\n",
    "In what follows, we implement the `sparse_crossentropy_loss` in three different ways. The first version (`my_CE_loss_1`) simply reuses the built-in loss of TF keras, while the second (`my_CE_loss_2`) and third (`my_CE_loss_3`) versions are more complicated and require us to manipulate the tensors of `y_true` and `y_pred`.\n",
    "- If you want to print out the values to inspect insightfully three versions, you can inject `tf.print(tensor)` to print out the tensor values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: reuse the CE loss we have learnt before\n",
    "class my_CE_loss_1(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(my_CE_loss_1, self).__init__()\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        loss =  tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits= False))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class my_CE_loss_2(tf.keras.losses.Loss):\n",
    "    def __init__(self, eps=1E-10, num_classes= 10):\n",
    "        super(my_CE_loss_2, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    # this is just the CE loss calculation\n",
    "    def call(self, y_true, y_pred):\n",
    "        idxs = tf.stack([tf.range(tf.shape(y_pred)[0]), tf.cast(tf.squeeze(y_true), tf.int32)], axis=0)\n",
    "         #use tf.print(idxs) here in case you want to print out values\n",
    "        y_pred_slice = tf.gather_nd(y_pred, tf.transpose(idxs))\n",
    "        loss = -tf.math.log(y_pred_slice + self.eps) \n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class my_CE_loss_3(tf.keras.losses.Loss):\n",
    "    def __init__(self, eps=1E-10, num_classes= 10):\n",
    "        super(my_CE_loss_3, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    # NOTE: this is just the CE loss calculation ; using one hot vector\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true_1_hot = tf.one_hot(tf.transpose(tf.cast(y_true, tf.int32), perm= [1,0])[0], depth= self.num_classes, axis=-1)\n",
    "        #use tf.print(y_true_1_hot) here in case you want to print out values\n",
    "        loss = -tf.math.multiply(y_true_1_hot, tf.math.log(y_pred + self.eps))\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">2.3. Define a custom weight initializer and regularizer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we write the code for our own Xavier and He initializations as a demonstration of how to implement our own weight initialization strategy. Similarly, you can custom l1 and l2 or your own regularizers with appropriate inputs and outputs. Please refer to [this link](https://keras.io/api/layers/initializers/) for more information regarding the initialization strategies supported by Keras Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we learnt two different techniques for weight initialization\n",
    "- if we use random weight, its not gonna help\n",
    "- if we have a strategy for initializing weight, we can learn better\n",
    "\n",
    "#### HE weight initializer\n",
    "- more suitable for ReLu\n",
    "\n",
    "#### Xavier weight initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def my_Xavier_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def my_He_initializer(shape, alpha=math.sqrt(2), dtype=tf.float32):\n",
    "    stddev = alpha*tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizer: reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def my_l2_regularizer(weights):\n",
    "    return tf.reduce_sum(0.01 * tf.square(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">2.4. Define a custom metric</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the existing metrics supported by Keras Tensorflow as in [this link](https://keras.io/api/metrics/), you can define your own metric of interest that fits your own purpose. Basically, you need to inherit from the class *tf.keras.metrics.Metric* and override the functions or methods: *\\__init\\__*, *update_state*, and *result*.\n",
    "\n",
    "Here we are going to implement the *top_k_accuracy* which returns a correct prediction if the top k predicted labels include the true label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: we can customize our own metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trying to calculate the top-k acuracy ; know the predictions / total data\n",
    "\n",
    "class TopkAcc(tf.keras.metrics.Metric):\n",
    "    def __init__(self, k=5, **kwargs):\n",
    "        super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
    "        self.k = k\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        y_true = tf.reshape(y_true, shape= [-1])\n",
    "        b_array = tf.math.in_top_k(y_true, y_pred, self.k)\n",
    "        num_corrects = tf.reduce_sum(tf.cast(b_array, tf.float32)) \n",
    "        self.total.assign_add(tf.reduce_sum(num_corrects))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "        \n",
    "    def result(self):\n",
    "        return self.total / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">3. Create our MiniVGG network</span> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create our MiniVGG network which applies the batch-norm, dropout techniques for mitigating the overfitting issue. We can also choose an initialization strategy for the weight matrices and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: activation is ReLu, initializer is Xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_vgg_model(n_classes=10):\n",
    "    vgg_model = models.Sequential()\n",
    "    \n",
    "    # # Other initialization strategies and different ways of passing parameters\n",
    "    # Conv2D(..., activation='elu', kernel_initializer='glorot_uniform', ...)\n",
    "    # Conv2D(..., activation=relu, kernel_initializer=my_He_initializer, ...)\n",
    "    # Dense(..., activation=tf.nn.selu, kernel_initializer=tf.keras.initializers.lecun_normal(), ...)\n",
    "    \n",
    "    vgg_model.add(layers.Conv2D(32, (3,3), padding='same', activation=relu, kernel_initializer=my_Xavier_initializer, input_shape=(32,32,3)))\n",
    "    vgg_model.add(layers.BatchNormalization(momentum=0.9))\n",
    "    vgg_model.add(layers.Conv2D(32, (3,3), padding='same', activation=relu, kernel_initializer=my_Xavier_initializer))\n",
    "    vgg_model.add(layers.BatchNormalization(momentum=0.9))\n",
    "    vgg_model.add(layers.MaxPool2D(pool_size=(2,2))) # downscale the image size by 2\n",
    "    vgg_model.add(layers.Dropout(rate=0.25)) # deactivate 25% of neurons for each feed-forward\n",
    "    vgg_model.add(layers.Conv2D(64, (3,3), padding='same', activation=relu, kernel_initializer=my_Xavier_initializer))\n",
    "    vgg_model.add(layers.BatchNormalization(momentum=0.9))\n",
    "    vgg_model.add(layers.Conv2D(64, (3,3), padding='same', activation=relu, kernel_initializer=my_Xavier_initializer))\n",
    "    vgg_model.add(layers.BatchNormalization(momentum=0.9))\n",
    "    vgg_model.add(layers.MaxPool2D(pool_size=(2,2))) # downscale the image size by 2\n",
    "    vgg_model.add(layers.Dropout(rate=0.25)) # deactivate 25% of neurons for each feed-forward\n",
    "    \n",
    "    # from here onwards, it is FC layer\n",
    "    vgg_model.add(layers.Flatten())\n",
    "    vgg_model.add(layers.Dense(512, activation=relu, kernel_initializer=my_Xavier_initializer))\n",
    "    vgg_model.add(layers.BatchNormalization(momentum=0.9))\n",
    "    vgg_model.add(layers.Dropout(rate=0.5)) \n",
    "    vgg_model.add(layers.Dense(n_classes, activation='softmax')) # ten classes in Cifar10\n",
    "    return vgg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the architecture of our MiniVGG network. Note that in the declaration of the *compile* method, we monitor two metrics of interest: *accuracy* and *TopkAcc* with $k=5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: To compile the vgg model, we can use our loss function, my_CE_loss_3\n",
    "- we are using own initializer\n",
    "- own loss function\n",
    "- own metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg_model = create_vgg_model(10)\n",
    "vgg_model.summary()\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "vgg_model.compile(optimizer=opt, loss=my_CE_loss_3(), metrics=[tf.keras.metrics.SparseCategoricalAccuracy(), TopkAcc(k=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg_model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">4. Learning Rate Scheduler</span> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUES: why must we make learning rate dynamic\n",
    "- we can probe the loss surface using different \"speeds\"\n",
    "- basically a grid search for the best learning rate for probing the surface area\n",
    "  - large value for learning rate will \"overshoot\"\n",
    "  - small learning rate learns too slow, might not get out of a saddle point in time ; there are performance issues as well\n",
    "  - we want the optimal learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scheduling learning rate is an important practical aspect of training deep learning models. Basically, we gradually drop the learning rate. When training a deep learning model, we do not aim to find global minimal points because this task is almost impossible due to the complexity of the loss surface. Instead, we decay the learning rate to reduce the oscillation in order to find the good-enough points with sufficiently small loss value (e.g., local minimum). If we set a high learning rate, your model would overshoot these points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we show how to implement a learning rate scheduler in TF 2.x. Our code of learning rate schedule is placed in the function **step_decay** which indicates the formula:\n",
    "- $lr = 0.01 \\times 0.25^{epoch/5}$, which decays along with training epochs. You can find other learning rate schedule strategies here: [learning rate schedule strategies](https://d2l.ai/chapter_optimization/lr-scheduler.html) for your own implementation.\n",
    "\n",
    "NOTE: in this case, will reduce the learning rat every 5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows the code for the learning rate schedule based on the current epoch and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def step_decay(epoch, learning_rate):\n",
    "    # initialize the base initial learning rate, drop factor, and epochs to drop every\n",
    "    init_lr = 0.01\n",
    "    factor = 0.25\n",
    "    drop_every = 5\n",
    "    # compute learning rate for the current epoch\n",
    "    learning_rate = init_lr*(factor ** (np.floor(epoch) / drop_every))\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now declare a callback for our **step_decay(epoch, learning_rate)** which helps us to rectify the learning rate during the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(step_decay)\n",
    "my_vgg = create_vgg_model(n_classes=10) # create the model\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01) # start learning rate at 0.01\n",
    "my_vgg.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# NOTE: use callbacks the specify the learning rate scheduler\n",
    "history = my_vgg.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, \n",
    "                     epochs=20, callbacks=[lr_scheduler], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we visualize the learning rate variation during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history[\"lr\"], \"o-\")\n",
    "plt.axis([0, 20 - 1, -0.0001, 0.012])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **<span style=\"color:red\">Exercise 1</span>:** Try with different learning rate schedulers as shown [here](https://d2l.ai/chapter_optimization/lr-scheduler.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">5. Spotting Underfitting/Overfitting and Early Stopping</span> ##\n",
    "\n",
    "stopping the training before overfitting occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training deep learning models, **observing and spotting underfitting/overfitting** are very important. At the very first epochs, because the model is not trained sufficiently, this suffers from **underfitting**, meaning that the existing model is too simple to characterize and fit the training data. However, when proceeding with the training procedure, the model becomes more complicated and fitting to the training dataset, hence possibly encountering **overfitting** at a certain point. \n",
    "\n",
    "We need to monitor the tendencies of training and validation losses to spot out the transmission of underfitting and overfitting for identifying a best-fit model.\n",
    "\n",
    "<img src=\"./images/underfit_overfit.png\" align=\"center\" width=600/>\n",
    "\n",
    "As shown in the above figure, we need to monitor and identify the epoch at which the validation loss starts increasing while the training loss is still decreasing. The model tends to transfer from underfitting to overfitting at this point, hence if we apply an **early stopping** here, we seem to reach the best-fit model.\n",
    "\n",
    "In what follows, we inspect how to implement the code to monitor the training, validation losses and accuracies to spot out their tendencies. Our treatment includes:\n",
    "- A json file to store the history of training. [Checkpoint]\n",
    "- A figure file to store the plots of training, validation losses and accuracies.\n",
    "- A model path to store the latest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import BaseLogger\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code supports us in saving a history H to a file. The history H is a dictionary of lists.\n",
    "- H={'loss':[2.5, 2.44, 2.3, 2.1], 'accuracy': [0.3, 0.42, 0.51, 0.56], 'val_loss':[2.7, 2.54, 2.44, 2.15], 'val_accuracy': [0.28, 0.41, 0.53, 0.55]}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_to_file(H=None, fig_path=None):\n",
    "    fig = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    x = [i+1 for i in range(len(H[\"loss\"]))] \n",
    "    plt.subplot(121)\n",
    "    plt.plot(x, H[\"loss\"], label=\"loss\")\n",
    "    plt.plot(x, H[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.title(\"Losses\")\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(x, H[\"accuracy\"], label=\"accuracy\")\n",
    "    plt.plot(x, H[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracies\")\n",
    "    \n",
    "    plt.savefig(fig_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare the class **TrainingMonitor** which inherits the class **keras.callbacks.BaseLogger**. The class **TrainingMonitor** has some attributes:\n",
    "- *fig_path* specifies where to store the plot of losses and accuracies.\n",
    "- *json_path* specifies where to store the history of losses and accuracies in the json format.\n",
    "- *model_path* specifies where to store our model. [NOTE: with all the weights we have trained previously]\n",
    "- *model* specifies the model object (i.e., an instance of our VGG model).\n",
    "\n",
    "In the callback function **on_train_begin** (triggered once when the training is started), we read the history in our json file to self.H, while in the callback function **on_train_end** (triggered once when the training is ended), we store the trained model to a file.\n",
    "\n",
    "In the callback function **on_epoch_end** (triggered at the end of each epoch), we take the information of the training/validation losses and accuracies via **logs** and then update the history self.H. Finally, we update the json file, and plot and save the plot of history to our figure file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TrainingMonitor(BaseLogger):\n",
    "    def __init__(self, fig_path=None, json_path=None, model_path=None, model=None):\n",
    "        super(TrainingMonitor, self).__init__()\n",
    "        self.fig_path = fig_path\n",
    "        self.json_path = json_path\n",
    "        self.model_path = model_path\n",
    "        self.model = model\n",
    "        \n",
    "    def on_train_begin(self, logs={}): # triggered when the training gets started\n",
    "        self.H = {} # initialize the history dictionary\n",
    "        if self.json_path is not None:   # if the JSON history path exists, load the training history\n",
    "            if os.path.exists(self.json_path):\n",
    "                self.H = json.loads(open(self.json_path).read())\n",
    "    \n",
    "    def on_train_end(self, logs={}): # triggered when the training gets ended\n",
    "        if self.model_path != None:\n",
    "            self.model.save_weights(self.model_path) # save the current model when finishing the training\n",
    "            \n",
    "        if len(self.H[\"loss\"]) > 0:\n",
    "            save_to_file(self.H, self.fig_path)\n",
    "            \n",
    "        if self.json_path is not None: # check to see if the training history should be serialized to file\n",
    "            f = open(self.json_path, \"w\")\n",
    "            f.write(json.dumps(self.H))\n",
    "            f.close()\n",
    "    \n",
    "    # on each epoch end callback method\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        for (k, v) in logs.items():   # loop over the logs and update the loss, accuracy, etc.\n",
    "            self.H.setdefault(k, [])            \n",
    "            self.H[k].append(str(round(v, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = \"./logs/section5/mini_vgg.h5\"\n",
    "fig_path = \"./logs/section5/history.png\"\n",
    "json_path = \"./logs/section5/history.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function **run_my_vgg**, we consider two cases:\n",
    "- The model has been trained from scratch\n",
    "  - We create a fresh new vgg model and use TraingMonitor to monitor the training.\n",
    "- The model has been retrained\n",
    "  - We load the existing model from the hard disk, train more, and use TraingMonitor to monitor the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run vgg model with 10 epochs\n",
    "def run_my_vgg(epochs=10):\n",
    "    my_vgg = create_vgg_model(n_classes=10) # Create the model\n",
    "    if os.path.exists(model_path):\n",
    "        my_vgg.load_weights(model_path) # Restore the weights\n",
    "        \n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    my_vgg.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    monitor = TrainingMonitor(fig_path=fig_path, json_path=json_path, model_path=model_path, model=my_vgg)\n",
    "    callbacks = [monitor]\n",
    "    my_vgg.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, \n",
    "               epochs=epochs, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first time training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_my_vgg(epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the figure after 10-epoch training. The gap between the loss values shows that the model has signs of overfitting\n",
    "\n",
    "obviously loss is inversely proportional to accuracy\n",
    "\n",
    "<img src=\"./images/history_10.png\" align=\"center\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try the second-time training with 10 epochs more to see if the model can be further improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_my_vgg(epochs=10) # NOTE: will load from model path and continue the training for another 10 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the following figure, the training becomes even worse with increasing validation loss and fluctuating validation accuracy.\n",
    "\n",
    "<img src=\"./images/history_20.png\" align=\"center\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">6. Early Stopping</span> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now demonstrate how to implement early stopping with TF 2.x. It is quite convenient with the built-in callback **EarlyStopping**. Here we set that we apply an early stopping if the validation loss is not decreasing two times in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# can declare an early stopping monitor based on the validation loss\n",
    "# mode = minimal ; \n",
    "# patience = 2 (patience is number of consecutive/subsequent epochs without any improvement then it will stop)\n",
    "early_checkpoint = EarlyStopping(patience=2, monitor='val_loss', mode='min')\n",
    "callbacks = [early_checkpoint]\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "vgg_model = create_vgg_model()\n",
    "vgg_model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "vgg_model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=20, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The early stopping is undertaken to save up our training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">7. Checkpointing Neural Network Model Improvements</span> ##\n",
    "\n",
    "We will be training the MiniVGGNet architecture on the CIFAR-10 dataset and then serializing our network weights to disk for the best models with either a **minimal validation loss** or **maximal validation accuracy**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg_model = create_vgg_model()\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01, decay=0.01/40, momentum=0.9, nesterov=True)\n",
    "vgg_model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we create two checkpoints:\n",
    "- *val_loss_checkpoint* to store the best model with the lowest validation loss.\n",
    "- *Val_acc_checkpoint* to store the best model with the highest validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# monitor this 1st checkpoint using validation loss\n",
    "val_loss_checkpoint = ModelCheckpoint(os.path.join(\"./ckpts\", \"best_val\"), monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1)\n",
    "# monitor this 2nd checkpoint using validation accuracy , keep poling to save the best model in terms of val accuracy\n",
    "val_acc_checkpoint = ModelCheckpoint(os.path.join(\"./ckpts\", \"best_acc\"), monitor=\"val_accuracy\", mode=\"max\", save_best_only=True, verbose=1)\n",
    "callbacks = [val_loss_checkpoint, val_acc_checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "H = vgg_model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=20, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load two best models from hard disk and then use them to evaluate the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# using the best model w.r.t accuracy to test using test set\n",
    "best_acc_model = load_model('./ckpts/best_acc')\n",
    "best_acc_model.compile(optimizer=\"sgd\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "best_acc_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using the best model w.r.t validation loss to test using test set\n",
    "best_val_model = load_model('./ckpts/best_val')\n",
    "best_val_model.compile(optimizer=\"sgd\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "best_val_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">8. Observing training progress using TensorBoard</span> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start this part which shows you how to use TensorBoard to diagnose your training behaviours: gradient vanishing and exploding. We recommend you install the following extensions to facilitate working with TensorBoard in jupyter notebook or lab:\n",
    "\n",
    "(1) TensorBoard extension for jupyter notebook: https://pypi.org/project/jupyter-tensorboard/\n",
    "\n",
    "(2) TensorBoard extension for jupyter lab: https://github.com/chaoleili/jupyterlab_tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TensorBoard in TF 2.x to monitor and visualize the training progress is very convenient. Basically, we declare a callback for TensorBoard as *tf.keras.callbacks.TensorBoard* with the following parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/tensorboard.png\" align=\"center\" width=700/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg_model = create_vgg_model(10)\n",
    "vgg_model.summary()\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "vgg_model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy', TopkAcc(k=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "log_dir = os.path.join('./logs/section8/tensorboard', 'logs_' + time.strftime('%Y-%m-%d_%H.%M.%S'))\n",
    "os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=True, histogram_freq=1, update_freq='epoch', write_images=True, profile_batch = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use the model with the tensorboard extension\n",
    "vgg_model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_valid, y_valid), callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to visualize the gradients using TensorBoard, you can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class GradientLogTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "    def __init__(self, log_dir='logs', histogram_freq=1, write_graph=False, write_images=False, update_freq='epoch', profile_batch=100000000, embeddings_freq=0, embeddings_metadata=None, num_instance= 100):\n",
    "        super(GradientLogTensorBoard, self).__init__(log_dir, histogram_freq, write_graph, write_images, update_freq, profile_batch, embeddings_freq, embeddings_metadata)\n",
    "        self.log_dir = log_dir\n",
    "        self.histogram_freq = histogram_freq\n",
    "        self.write_graph = write_graph\n",
    "        self.write_images= write_images\n",
    "        self.update_freq = update_freq\n",
    "        self.profile_batch = profile_batch\n",
    "        self.embeddings_freq = embeddings_freq\n",
    "        self.embeddings_metadata = embeddings_metadata\n",
    "        self.num_instance = num_instance\n",
    "    \n",
    "    def log_gradient(self, epoch):\n",
    "        idxs = np.random.choice(X_train.shape[0], self.num_instance, replace= False)\n",
    "        X_batch = X_train[idxs]\n",
    "        y_batch = y_train[idxs]\n",
    "        \n",
    "        writer = self._train_writer\n",
    "        # writer = self._get_writer(self._train_run_name)\n",
    "        # writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        with writer.as_default(), tf.GradientTape() as g:\n",
    "            g.watch(tf.convert_to_tensor(X_batch, dtype=tf.float64))\n",
    "            _y_pred = self.model(X_batch)  # forward-propagation\n",
    "            loss = tf.losses.sparse_categorical_crossentropy(y_batch, _y_pred)  # calculate loss\n",
    "            gradients = g.gradient(loss, self.model.trainable_weights)  # back-propagation\n",
    "\n",
    "            # In eager mode, grads does not have name, so we get names from model.trainable_weights\n",
    "            for weights, grads in zip(self.model.trainable_weights, gradients):\n",
    "                tf.summary.histogram(weights.name.replace(':', '_') + '_grads_epoch_' + str(epoch), data=grads, step=epoch)\n",
    "                tf.summary.scalar(weights.name.replace(':', '_') + '_grads_norm_epoch_' + str(epoch), data=tf.norm(grads), step=epoch)\n",
    "        writer.flush()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super(GradientLogTensorBoard, self).on_epoch_end(epoch, logs=logs)\n",
    "        if self.histogram_freq:\n",
    "            self.log_gradient(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "log_dir = os.path.join('./logs/section8/gradient', 'logs_' + time.strftime('%Y-%m-%d_%H.%M.%S'))\n",
    "os.makedirs(log_dir)\n",
    "gradient_log = GradientLogTensorBoard(num_instance=32, histogram_freq=1, update_freq='epoch', write_images=True, profile_batch=100000000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use other callback, gradient_log\n",
    "vgg_model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_valid, y_valid), callbacks=[gradient_log])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 2</span>:** Observing and monitoring the gradient norm is very important to see if your training encounters gradient vaninishing. Write your code to plot the gradient norm of the model w.r.t. all trainable parameters during the training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf2_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "15d2ba40478b3ac6cd56796f62289265aeaa4b1060bcfd262a3fbfdb2f399e72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
